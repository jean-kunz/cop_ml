{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn_dinos.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyM50guG+xdkw/bL8OmcTBOS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2NDDNDVT62rO","colab_type":"code","colab":{}},"source":["#!pip install kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEjB79rjN_JL","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U06b_W-s4qJd","colab_type":"text"},"source":["# Get the data\n","\n","upload the dinos.txt file into /content directory from kaggle dataset"]},{"cell_type":"code","metadata":{"id":"mJBH-Sc3znKg","colab_type":"code","colab":{}},"source":["!mkdir /content/.kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"974fYi-17v21","colab_type":"code","colab":{}},"source":["# get the keys from your kaggle account information\n","# my account > API > Create New API Token\n","# get the content of the file \n","import json\n","token = {\"username\":\"jeankunz\",\"key\":\"94c94aa22b3a009d7e6e48bbd45bd6ee\"}\n","with open('/content/.kaggle/kaggle.json', 'w') as file:\n","    json.dump(token, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1Hj9RZj7WPb","colab_type":"code","colab":{}},"source":["%%bash\n","\n","if [ ! -f /content/dinos.txt ]; then\n","    mkdir /root/.kaggle\n","    cp /content/.kaggle/kaggle.json /root/.kaggle/\n","    cat /root/.kaggle/kaggle.json\n","    kaggle config set -n path -v{/content}\n","    kaggle datasets download -d swimmingwhale/dinosaur-island  -p /content\n","    \n","    unzip -o /content/dinosaur-island.zip -d /content\n","fi\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDKH-VCC8XsC","colab_type":"text"},"source":["# Data preparation"]},{"cell_type":"code","metadata":{"id":"Np0c2Yex1bPW","colab_type":"code","colab":{}},"source":["data = open('/content/dinos.txt', 'r').read()\n","data= data.lower()\n","chars = list(set(data))\n","data_size, vocab_size = len(data), len(chars)\n","print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REz74f4sA6XA","colab_type":"code","colab":{}},"source":["dino_list = list(data.split('\\n'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AEHN6dnl9Sra","colab_type":"text"},"source":["The characters are a-z (26 characters) plus the \"\\n\" (or newline character), which in this assignment plays a role similar to the <EOS> (or \"End of sentence\") token we had discussed in lecture, only here it indicates the end of the dinosaur name rather than the end of a sentence. In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character character. This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. Below, char_to_ix and ix_to_char are the python dictionaries."]},{"cell_type":"code","metadata":{"id":"urK0gMa71FfI","colab_type":"code","colab":{}},"source":["char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n","ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n","print(ix_to_char)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gln6WkfW9sk6","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","from absl import logging"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jg5GmceP3eeG","colab_type":"code","colab":{}},"source":[" # Build list of all dinosaur names (training examples).\n","with open(\"dinos.txt\") as f:\n","    examples = f.readlines()\n","examples = [x.lower().strip()+\"\\n\" for x in examples]\n","\n","# Shuffle list of all dinosaur names\n","np.random.seed(0)\n","np.random.shuffle(examples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcQscsRqCu-v","colab_type":"code","colab":{}},"source":["examples[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3r55JXk5gA_","colab_type":"code","colab":{}},"source":["def string_to_ixs(str):\n","    return [char_to_ix[c] for c in str]\n","\n","def ixs_to_string(ixs):\n","    chars= [ix_to_char[ix] for ix in ixs]\n","    return \"\".join(chars)\n","\n","ixs = string_to_ixs(\"hello\")\n","assert 'hello'==ixs_to_string(ixs)\n","\n","'''\n","def string_to_X(str, max_len,char_to_ix):\n","    nb_zeros_to_add = max_len - (len(str)+1)    \n","    X = [char_to_ix[ch] for ch in str] \n","    X_padded = X + [0]*nb_zeros_to_add\n","\n","    return X_padded\n","\n","def string_to_Y(str, max_len,char_to_ix):\n","    # we add 1 to maxlen because we always add a last char of \\n\n","    max_len+=1\n","    nb_zeros_to_add = max_len - (len(str)+1)\n","    Y = [char_to_ix[ch] for ch in str] + [char_to_ix[\"\\n\"]]\n","    Y_padded = Y + [0]*nb_zeros_to_add\n","  \n","    return Y_padded\n","'''\n","def create_x_y(str):\n","    #nb_zeros_to_add = max_len - (len(str)+1)\n","    str_ix = [char_to_ix[ch] for ch in str]\n","    X = str_ix[:-1]\n","    Y = str_ix[1:]\n","    return X,Y\n","\n","\n","create_x_y(\"hello\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ug76FThIw9fY","colab_type":"code","colab":{}},"source":["#example_ds = tf.data.Dataset.from_tensor_slices(examples)\n","#next(iter(example_ds.take(1)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-GeRbbpr1cpM","colab_type":"code","colab":{}},"source":["dino_max_len = len(max(examples, key=len))+1\n","X = []\n","Y = []\n","for dino in examples:\n","    x,y = create_x_y(dino)\n","    X.append(x)\n","    Y.append(y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tmlGzRRJRw_","colab_type":"code","colab":{}},"source":["X[:2], Y[:2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YfGTTCiNZxvG","colab_type":"code","colab":{}},"source":["padded_X = tf.keras.preprocessing.sequence.pad_sequences(X,maxlen=dino_max_len,padding='post')\n","padded_Y = tf.keras.preprocessing.sequence.pad_sequences(Y,maxlen=dino_max_len,padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jcigtpg9mWU","colab_type":"code","colab":{}},"source":["# we don't padd because we want the model to be dynamic, with no fixed length\n","data_ds = tf.data.Dataset.from_tensor_slices((padded_X,padded_Y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t2Xy5U_Wqq1X","colab_type":"text"},"source":["RNN model\n","\n","![Texte alternatifâ€¦](https://www.tensorflow.org/tutorials/text/images/text_generation_training.png)\n","\n","Diff between vanilla RNN, GRU, LSTM: https://medium.com/@saurabh.rathor092/simple-rnn-vs-gru-vs-lstm-difference-lies-in-more-flexible-control-5f33e07b1e57\n","\n","https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9"]},{"cell_type":"code","metadata":{"id":"z--tku6gAOVo","colab_type":"code","colab":{}},"source":["seq_len = dino_max_len\n","embedding_size = 32\n","rnn_units= 128\n","batch_size = 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvUoHzfR_ffS","colab_type":"code","colab":{}},"source":["seq_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CSF_SKgxAOnb","colab_type":"code","colab":{}},"source":["def build_model(embedding_size, rnn_units,vocab_size):\n","    # No constraint on batch_size and seq_len in model\n","    inputs = tf.keras.Input(batch_shape=(None, None))\n","    x = tf.keras.layers.Embedding(vocab_size,embedding_size)(inputs)\n","    # stateful is useful when we want to keep the state (s, in case of lstm) (not the weight) between samples, \n","    #like when we learn from a very long sequence (a book for instance) that we split.\n","    x = tf.keras.layers.GRU(rnn_units,return_sequences=True, stateful=False,recurrent_initializer='glorot_uniform')(x)\n","    #x = tf.keras.layers.LSTM(rnn_units,return_sequences=True, stateful=False,recurrent_initializer='glorot_uniform')(x)\n","    outputs = tf.keras.layers.Dense(vocab_size)(x)\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='char_generation_model')\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLgm8WveAOsn","colab_type":"code","colab":{}},"source":["model = build_model(embedding_size, rnn_units,vocab_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VJfIy4JoPx9Z","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZsH798tAOv3","colab_type":"code","colab":{}},"source":["# We train on all data, because we don't want to predict but to generate new names\n","train_batch_ds = data_ds.shuffle(buffer_size=data_size).batch(batch_size,drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"boc6aM7d8fd3","colab_type":"code","colab":{}},"source":["input_batch, output_batch = next(iter(train_batch_ds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5cWgPGUlZ1L","colab_type":"code","colab":{}},"source":["output_batch.numpy().shape, input_batch.numpy().shape\n","input_batch[:2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9p5eHvWw8fg6","colab_type":"code","colab":{}},"source":["# we try to predict, even if we have not trained it. it output a shape of batch_size, seq_len (dino_max_len), vocab_size\n","preds = model.predict(input_batch)\n","# preds shape is batch_size, seq_len (dino_max_len), vocab_size\n","pred_names = []\n","for pred in preds[:1]:\n","    # We sample from distribution that has been computed by RNN\n","    sampled_indices = tf.random.categorical(pred, num_samples=1)  \n","    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()  \n","    pred_names.append(ixs_to_string(sampled_indices))\n","\n","pred_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKjZiY6veeRH","colab_type":"text"},"source":["# Training on examples"]},{"cell_type":"code","metadata":{"id":"EP2bUOky8fn5","colab_type":"code","colab":{}},"source":["# select a loss function\n","# as our model returns a logit, not a softmax one need to set from_logits=True\n","def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","example_batch_loss  = loss(output_batch, preds)\n","example_batch_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"deaJbufg8fq5","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam', loss=loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kh03teUU8fvX","colab_type":"code","colab":{}},"source":["import os\n","import datetime\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_cb=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","log_dir=\"logs/rnn-dino-fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True,\n","                                                      embeddings_freq=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKOg1-E-8ft2","colab_type":"code","colab":{}},"source":["history = model.fit(\n","    train_batch_ds, \n","    callbacks=[checkpoint_cb,tensorboard_cb],\n","    epochs=200)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciVc2wI_VrJF","colab_type":"code","colab":{}},"source":["tf.debugging.experimental.enable_dump_debug_info(\"logs/rnn-dino-fit/\", tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZU4j8BFETGJ","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs/rnn-dino-fit/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRDVRElPo32D","colab_type":"code","colab":{}},"source":["input_batch, output_batch = next(iter(train_batch_ds))\n","inputs = input_batch.numpy()\n","outputs = output_batch.numpy()\n","input_names = []\n","output_names = []\n","for input_idx in inputs[:5]:\n","    input_names.append(ixs_to_string(input_idx))\n","\n","for output_idx in outputs[:5]:\n","    output_names.append(ixs_to_string(output_idx))\n","\n","input_names, output_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjAk5KW4R2sO","colab_type":"code","colab":{}},"source":["input_batch.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxzXn2e9oC3i","colab_type":"code","colab":{}},"source":["# do predictions based on training input. this is not name generation, because we provide as input a real name.\n","preds = model.predict(input_batch)\n","# preds shape is batch_size, seq_len (dino_max_len), vocab_size\n","pred_names = []\n","input_names = []\n","\n","input_np = input_batch.numpy()\n","\n","for i, pred in enumerate(preds[:2]) :\n","    # We sample from distribution that has been computed by RNN\n","    sampled_indices = tf.random.categorical(pred, num_samples=1)    \n","    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()    \n","    pred_names.append(ixs_to_string(sampled_indices))\n","    input_names.append(ixs_to_string(input_np[i]))\n","\n","input_names, pred_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xiAqyz_BqJJq","colab_type":"text"},"source":["# Name generation\n","\n","![Name generation](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png )"]},{"cell_type":"code","metadata":{"id":"K-Ytyp82oC6h","colab_type":"code","colab":{}},"source":["# we feed into the model one char after the other\n","\n","def generate_name(model, start_string=None, seq_len=None):\n","    generated_name = []\n","    start_ixs = string_to_ixs(start_string.lower())\n","    input_idx = tf.expand_dims(start_ixs, 0) \n","    print(input_idx)   \n","    #model.reset_states()\n","\n","    #print(input.shape)\n","    # essayer en accumulant dans input data le start + les char. \n","    for i in range(seq_len):\n","        pred = model.predict(input_idx)\n","        \n","        # remove the batch dimension\n","        pred = tf.squeeze(pred, 0)\n","        #print(\">\", pred)\n","        #print(\"pred\", pred.shape)\n","        #print(\"pred last\", tf.random.categorical(pred, num_samples=1).numpy())\n","\n","        # input : batch_size, num_clas\n","        predicted_id = tf.random.categorical(pred, num_samples=1)[-1,0].numpy()\n","        predicted_char = ix_to_char[predicted_id]\n","        predicted_idx = tf.expand_dims([predicted_id], 0)\n","        input_idx = tf.concat([input_idx, predicted_idx], axis=1)\n","        #print(input_idx)\n","        if predicted_char is '\\n':\n","            break\n","        generated_name.append(predicted_char)\n","        \n","    name = (start_string + ''.join(generated_name))\n","    return name, len(name)\n","\n","\n","generate_name(model, start_string=\"k\", seq_len=40)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lbj0KGQMVHcm","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}