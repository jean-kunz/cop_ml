{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "var_prediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1WwnPgkKXrgIZHqFX4QHY1KAlftmIwMpc",
      "authorship_tag": "ABX9TyNjE5XWB0xL/4Vsuzv331Wr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jean-kunz/cop_ml/blob/master/var_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x2ENxdB9bJs"
      },
      "source": [
        "# quantile regression\n",
        "\n",
        "In ordinary linear regression, we are estimating the mean of some variable y, conditional on the values of independent variables X. \n",
        "\n",
        "As we proceed to fit the OLS regression model on the data we make a __key assumption about the random error term__ in the linear model. Our assumption is that the __error term has a constant variance__ across the values of independent variable X.\n",
        "\n",
        "- What happens when this assumption is no longer true ?\n",
        "- Also instead of estimating the mean of our independent variable can we estimate the median or the 0.3th quantile or 0.8th quantile of our independent variable ?.\n",
        "\n",
        "This is where Quantile Regression comes to our rescue : A linear model to predict a given quantile of y, not the mean as in OLS \n",
        "\n",
        "__Quantile regression estimates the conditional median (or quantile) of the target__. Quantile regression is an extension of linear regression that is used when the conditions of linear regression are not met (i.e., linearity, homoscedasticity, independence, or normality).\n",
        "\n",
        "For example (in a price prediction model), if we were to find the 25th quantile for the price of a particular home, that would mean that there is a 25% chance the actual price of the house is below the prediction, while there is a 75% chance that the price is above.\n",
        "\n",
        "Now instead of being constants as in OLS, the regression coefficients ($\\Theta$ or $\\beta$) are now functions with a dependency on the quantile. For each quantile there are other coefs.\n",
        "\n",
        "https://towardsdatascience.com/quantile-regression-ff2343c4a03\n",
        "\n",
        "https://www.datasciencecentral.com/profiles/blogs/quantile-regression-in-python\n",
        "\n",
        "### implementation\n",
        "https://www.statsmodels.org/dev/examples/notebooks/generated/quantile_regression.html\n",
        "\n",
        "https://colab.research.google.com/drive/1nXOlrmVHqCHiixqiMF6H8LSciz583_W2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCuHmSxEeqGc"
      },
      "source": [
        "!pip install line_profiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgIfiq9Ze_d3"
      },
      "source": [
        "%load_ext line_profiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyZ2PZ-0w-Sv"
      },
      "source": [
        "\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = sm.datasets.engel.load_pandas().data\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HVr8texxLDG"
      },
      "source": [
        "mod = smf.quantreg('foodexp ~ income', data)\n",
        "res = mod.fit(q=.5)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dugfjO7exe8Z"
      },
      "source": [
        "quantiles = np.arange(.05, .96, .1)\n",
        "#quantiles = np.array([0.5])\n",
        "def fit_model(q):\n",
        "    #print(mod)    \n",
        "    # fit with quantile regr model : mod\n",
        "    res = mod.fit(q=q)\n",
        "    return [q, res.params['Intercept'], res.params['income']] + \\\n",
        "            res.conf_int().loc['income'].tolist()\n",
        "\n",
        "models = [fit_model(x) for x in quantiles]\n",
        "models = pd.DataFrame(models, columns=['q', 'intercept', 'income', 'lower_ci_bound', 'upper_ci_bound'])\n",
        "\n",
        "ols_model = smf.ols('foodexp ~ income', data).fit()\n",
        "ols_ci = ols_model.conf_int().loc['income'].tolist()\n",
        "ols = dict(intercept = ols_model.params['Intercept'],\n",
        "           income = ols_model.params['income'],\n",
        "           lower_ci_bound = ols_ci[0],\n",
        "           upper_ci_bound = ols_ci[1])\n",
        "\n",
        "print(\"quantile regr: \\n\",models)\n",
        "print(\"ols: \\n\",ols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhFWfSZb5ie5"
      },
      "source": [
        "data.quantile(quantiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j6h76L87f1v"
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N915SZzHxLQa"
      },
      "source": [
        "x = np.arange(data.income.min(), data.income.max(), 50)\n",
        "get_y = lambda a, b: a + b * x\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "for i in range(models.shape[0]):\n",
        "    y = get_y(models.intercept[i], models.income[i])\n",
        "    #print(\"quantile:\", quantiles[i])\n",
        "    if 0.3<quantiles[i]<0.6:\n",
        "        ax.plot(x, y, linestyle='dotted', color='black', label=quantiles[i])\n",
        "    else:\n",
        "        ax.plot(x, y, linestyle='dotted', color='lightblue')\n",
        "    \n",
        "\n",
        "y = get_y(ols['intercept'], ols['income'])\n",
        "\n",
        "ax.plot(x, y, color='red', label='OLS')\n",
        "ax.scatter(data.income, data.foodexp, alpha=.2)\n",
        "ax.set_xlim((240, 3000))\n",
        "ax.set_ylim((240, 2000))\n",
        "legend = ax.legend()\n",
        "ax.set_xlabel('Income', fontsize=16)\n",
        "ax.set_ylabel('Food expenditure', fontsize=16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsoXOgD975x7"
      },
      "source": [
        "n = models.shape[0]\n",
        "p1 = plt.plot(models.q, models.income, color='black', label='Quantile Reg.')\n",
        "p2 = plt.plot(models.q, models.upper_ci_bound, linestyle='dotted', color='black')\n",
        "p3 = plt.plot(models.q, models.lower_ci_bound, linestyle='dotted', color='black')\n",
        "p4 = plt.plot(models.q, [ols['income']] * n, color='red', label='OLS')\n",
        "p5 = plt.plot(models.q, [ols['lower_ci_bound']] * n, linestyle='dotted', color='red')\n",
        "p6 = plt.plot(models.q, [ols['upper_ci_bound']] * n, linestyle='dotted', color='red')\n",
        "plt.ylabel(r'$\\beta_{income}$')\n",
        "plt.xlabel('Quantiles of the conditional food expenditure distribution')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoV4-M5O76IR"
      },
      "source": [
        "## Value at risk prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beCKOnQpA80V"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from absl import logging\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import scipy\n",
        "import numpy as np\n",
        "#%reload_ext google.colab.data_table\n",
        "from IPython.display import display as dsp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox0zDngGD3Gc"
      },
      "source": [
        "ls /content/drive/My\\ Drive/Colab\\ Notebooks/cop_ml/dataset/fh_5yrs.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3cMb04C3Fpg"
      },
      "source": [
        "\n",
        "Here is the paper we want to reproduce: https://arxiv.org/pdf/1908.07978v1.pdf\n",
        "\n",
        "Data are located at kaggle: \n",
        "https://www.kaggle.com/qks1lver/amex-nyse-nasdaq-stock-histories?select=fh_5yrs.csv\n",
        "\n",
        "We take the fh_5yrs.csv file that is uploaded to google drive. Then google drive is installed within collaboratory environment so one can access it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7MxcWolA8KQ"
      },
      "source": [
        "stocks_path = '/content/drive/My Drive/Colab Notebooks/cop_ml/dataset/fh_5yrs.csv'\n",
        "stocks_df = pd.read_csv(stocks_path)\n",
        "stocks_df.date = pd.to_datetime(stocks_df.date)\n",
        "#%unload_ext google.colab.data_table\n",
        "#stocks_df = stocks_df.loc[stocks_df.symbol.isin(['AAPL','TSLA'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lQa3DxRu-Eg"
      },
      "source": [
        "stocks_df['returns'] = stocks_df.sort_values(['symbol','date'], ascending=True).groupby(['symbol']).adjclose.pct_change()\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(10,3)\n",
        "sns.lineplot(data=stocks_df.query(\"symbol=='AAPL'\").sort_values('date', ascending=True), x=\"date\", y=\"adjclose\")\n",
        "# first return of each symbol is nan -> set to 0\n",
        "stocks_df.returns.fillna(0., inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTCvZDIBtm4i"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_size_inches(10,3)\n",
        "\n",
        "sns.lineplot(data=stocks_df.query('symbol==\"AAPL\"').sort_values('date', ascending=True), x='date', y='returns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1K1Jyz1QKLR"
      },
      "source": [
        "stocks_df.query('symbol==\"AAPL\"').returns.hist(bins=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY13pyyFwNzV"
      },
      "source": [
        "Q = stocks_df.query('symbol==\"AAPL\"').returns.dropna()\n",
        "scipy.stats.probplot(Q, dist=scipy.stats.norm, plot=plt.figure().add_subplot(111))\n",
        "plt.title(\"Normal QQ-plot\", weight=\"bold\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svv0OZO9wN2R"
      },
      "source": [
        "tdf, tmean, tsigma = scipy.stats.t.fit(Q)\n",
        "scipy.stats.probplot(Q, dist=scipy.stats.t, sparams=(tdf, tmean, tsigma), plot=plt.figure().add_subplot(111))\n",
        "plt.title(\"Student QQ-plot\", weight=\"bold\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0DN4n8aEleC"
      },
      "source": [
        "### Normalization\n",
        "\n",
        "https://developers.google.com/machine-learning/data-prep/transform/normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hmk4XXibtmV"
      },
      "source": [
        "# a few outliers\n",
        "#plt.hist(stocks_df.adjclose, bins=20)\n",
        "sns.histplot(data=stocks_df, x='adjclose', bins=20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQbL_dbbbTbp"
      },
      "source": [
        "stocks_df.loc[:,'log_adjclose']= np.log(stocks_df.adjclose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meYhHR7NfFH4"
      },
      "source": [
        "# log absorb outliers\n",
        "sns.histplot(data=stocks_df, x='log_adjclose', bins=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZInW1O2d8JJ"
      },
      "source": [
        "# normalize with z-score\n",
        "stocks_df['z_adjclose']=np.divide((stocks_df.adjclose - stocks_df.adjclose.mean()),stocks_df.adjclose.std())\n",
        "#stocks_df.adjclose.std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB1lNxAOd8TU"
      },
      "source": [
        "sns.histplot(data=stocks_df, x='z_adjclose', bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TolSAFJqDHXf"
      },
      "source": [
        "log scaling looks like the one we should use\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbbOBTeH49hK"
      },
      "source": [
        "from sklearn.preprocessing import power_transform\n",
        "stocks_df.loc[:,'log_returns'] = power_transform(np.expand_dims(stocks_df.returns, axis=1),method='yeo-johnson')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKnEySAw5Kr9"
      },
      "source": [
        "#sns.histplot(data=stocks_df, x='log_returns', bins=30)\n",
        "sns.histplot(data=stocks_df, x='log_returns', bins=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Vx6P17nAVW"
      },
      "source": [
        "## Returns vs Adjclose correlation\n",
        "\n",
        "Check if they are correlated because both are candidate to be features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_dJ8ZLnm_Z0"
      },
      "source": [
        "stocks_df.query(\"symbol=='TSLA'\")[['log_adjclose','returns']].corr()\n",
        "stocks_df[['log_adjclose','returns']].corr()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP8CH1Czw8p3"
      },
      "source": [
        "# definition of VAR\n",
        "\n",
        "If the 0.05 empirical quantile of daily returns is at 0.11. That means that with 95% confidence, our worst daily loss will not exceed 11%. If we have a 1 M€ investment, our one-day 5% VaR is 0.11 * 1 M€ = 34 k€.\n",
        "\n",
        "Here we want to predict the maximum loss we can have with a confidence level of $1-\\theta$, $\\theta$ being the confidence level.\n",
        "\n",
        "http://www.simonemanganelli.org/Simone/Research_files/caviarPublished.pdf\n",
        "\n",
        "Traditional var assumes distribution of return doesn't change over time, at least during return measurement. \n",
        "\n",
        "It fit returns to a distribution (mean, std,...) and use quantile to define the thresehold perf. \n",
        "\n",
        "Here we try to predict VAR with a different method than historical var, which use distribution of daily return and returns the theta quantile of the worst daily pct returns\n",
        "\n",
        "**We try to predict what will be the worst perf in the next 30 days**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAARrBvDHsNt"
      },
      "source": [
        "stocks_df.columns\n",
        "stocks_df.symbol.unique()\n",
        "symbol_list = stocks_df.symbol.unique()\n",
        "np.random.shuffle(symbol_list)\n",
        "symbol_list = symbol_list.tolist()[:100]\n",
        "symbol_list.extend(['AAPL','TSLA'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1fHuseuF2m3"
      },
      "source": [
        "df = stocks_df.loc[stocks_df.symbol.isin(symbol_list)]\n",
        "#df = stocks_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlFhP2bOYUxt"
      },
      "source": [
        "df.date.max(), df.date.min(), stocks_df.date.max(), stocks_df.date.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufihncMs-IM5"
      },
      "source": [
        "# compute the max loss (min neg returns) per stock on expanding window.\n",
        "max_loss_df = df.sort_values(['symbol','date']).groupby(['symbol']) \\\n",
        "                    .expanding(1)[['returns']].min().reset_index()\n",
        "max_loss_df.rename(columns={'returns':'max_loss'}, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpoVdTYUApn"
      },
      "source": [
        "# to do a forward rolling window to find the next worst perf in 30 timesteps.\n",
        "# we do sort by date in reverse order with a rolling window.  \n",
        "max_loss_30_df = df.sort_values(['symbol','date'], ascending=False).groupby(['symbol']) \\\n",
        "                    .rolling(30, on='date', min_periods=1)[['returns']].min().reset_index()\n",
        "max_loss_30_df.rename(columns={'returns':'max_loss_30'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI6XLgPc920P"
      },
      "source": [
        "max_loss_30_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHNRCVW4_I4E"
      },
      "source": [
        "all_df = df.merge(max_loss_30_df, on=['symbol','date'], how='inner')\n",
        "all_df.fillna(0.0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghVgkxa-tg0m"
      },
      "source": [
        "data_to_display = all_df.query('symbol==\"AAPL\"')[['date','returns','max_loss_30']]\n",
        "sns.lineplot(x='date', y='value', hue='variable',\n",
        "             data=pd.melt(data_to_display, ['date']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjjH6qfJn8BY"
      },
      "source": [
        "from datetime import date\n",
        "\n",
        "bd = pd.tseries.offsets.BusinessDay(n = 30) \n",
        "date(2015,1,5) + bd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S72Yuvw1jS8V"
      },
      "source": [
        "all_df[(all_df.symbol=='AAPL')&(all_df.returns<0)].sort_values('date')[['date','returns','max_loss_30']].head(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp0_8Gp-xp4d"
      },
      "source": [
        "# implement a vectorized version\n",
        "\n",
        "def rolling_win_padded(x, win_len):\n",
        "    nb_row = x.shape[0]\n",
        "    nb_col = x.shape[1]\n",
        "    #print(\"nb rows in rolling padded\", nb_row)\n",
        "\n",
        "    # pad x with zeros left of win_len - 1 (there is at least one row in window on the right)\n",
        "    pad = np.zeros((win_len-1, nb_col ))\n",
        "    padded_x = np.concatenate([pad,x])\n",
        "    #print(\"padded x\", padded_x, \"\\n pad\", pad)\n",
        "    # create a vectorized index based on a rolling index\n",
        "    # rolling index defines windows with index pointing to data in x\n",
        "    win_dim = np.expand_dims(np.arange(win_len),0)\n",
        "    timestep_dim = np.expand_dims(np.arange(nb_row),0).T\n",
        "    rolling_idx = win_dim + timestep_dim\n",
        "    #print(\"rolling idx\", rolling_idx, \"\\n win dim\", win_dim,\"\\n ts dim\" , timestep_dim)\n",
        "    return padded_x[rolling_idx]\n",
        "\n",
        "\n",
        "def rolling_future_value(x, nb_future_steps=1):\n",
        "    nb_row = x.shape[0]\n",
        "    nb_col = x.shape[1]\n",
        "    #print(\"nb rows in future value\", nb_row)\n",
        "\n",
        "    #print(np.expand_dims(np.arange(nb_future_steps),0))\n",
        "    # standard case when there are more rows than future steps\n",
        "    if nb_row >= nb_future_steps:\n",
        "        base_idx = np.arange(nb_row-nb_future_steps)\n",
        "        #print(\"base_idx\",base_idx)\n",
        "        future_idx = base_idx + nb_future_steps\n",
        "        #print(\"future_idx\", future_idx)    \n",
        "        end_pad = np.zeros((nb_future_steps, nb_col))\n",
        "        futur_x = x[future_idx]\n",
        "        #print(x.shape, futur_x.shape, end_pad.shape)\n",
        "        end_padded_x = np.concatenate([futur_x, end_pad])\n",
        "    else:\n",
        "        # return nb_row of zeros\n",
        "        end_padded_x = np.zeros((nb_row, nb_col))\n",
        "    return end_padded_x\n",
        "\n",
        "\n",
        "x = np.array([[1,'a',1.5],\n",
        "             [2, 'b', 3.2],\n",
        "             [3, 'c', 3.5],\n",
        "             [4, 'd', 3.3],\n",
        "             [5, 'e', 5.2],\n",
        "             [6, 'f', 8.2]])\n",
        "\n",
        "rolling_x = rolling_win_padded(x[:3,:], 4)\n",
        "\n",
        "#rolling_y = rolling_future_value(np.expand_dims(x[:,2],1), 2)\n",
        "#rolling_y = rolling_future_value(x[:3,2:3], 5)\n",
        "\n",
        "# if x.shape[0] < nb_fut_steps, it should return x.shape[0] \n",
        "rolling_y = rolling_future_value(x[:3,2:3], 1)\n",
        "\n",
        "print(\"rolling x: \\n\",rolling_x,\"\\n rolling y\\n\" ,rolling_y)\n",
        "assert rolling_x.shape[0]== rolling_y.shape[0], \"should return same nb of rows\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upEztT0wGJ4x"
      },
      "source": [
        "assert len(all_df[all_df.returns.isnull()==True])==0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGBfUBhexWTX"
      },
      "source": [
        "def rolling_win_vec(df, **kwargs):       \n",
        "    input_cols = kwargs['input_cols']\n",
        "    output_col = kwargs['output_col']   \n",
        "    win_len = kwargs['win_len']\n",
        "    sort_col_name = kwargs['sort_col_name']\n",
        "    nb_pred_timesteps = kwargs['nb_pred_timesteps']\n",
        "    #print(\">> symbol:\",df.symbol.unique()[0])\n",
        "    x = np.array(df.sort_values(sort_col_name)[input_cols].values)\n",
        "    y = np.array(df.sort_values(sort_col_name)[output_col].values)\n",
        "    padded_win_x =  rolling_win_padded(x, win_len)    \n",
        "    padded_y = rolling_future_value(y, nb_future_steps=nb_pred_timesteps)\n",
        "    #print(len(df),y.shape, padded_win_x.shape, padded_y.shape)\n",
        "\n",
        "    df['rolling_x']= padded_win_x.tolist()\n",
        "    df['y']= padded_y.tolist()\n",
        "    #print(\">>\", padded_win_x)\n",
        "    return df\n",
        "\n",
        "nb_pred_timesteps = 0 #we predict for the next 30 steps (open days) the max loss\n",
        "\n",
        "def prep_features(df, symbols=None):\n",
        "    if symbols is not None:\n",
        "        grp_df = all_df.loc[all_df.symbol.isin(symbols)].sort_values(['symbol','date']).groupby('symbol')\n",
        "    else:\n",
        "        grp_df = all_df.sort_values(['symbol','date']).groupby('symbol')\n",
        "    \n",
        "    win_df = grp_df.apply(rolling_win_vec, input_cols=['log_adjclose','returns'],\n",
        "                                              output_col=['max_loss_30'],\n",
        "                                              win_len=128,\n",
        "                                              nb_pred_timesteps = nb_pred_timesteps,\n",
        "                                              sort_col_name='date')\n",
        "    \n",
        "    return win_df\n",
        "\n",
        "symbol_list = all_df.symbol.unique()\n",
        "np.random.shuffle(symbol_list)\n",
        "symbols = [symbol_list[0]]\n",
        "symbols = symbol_list[:]\n",
        "stock_win_df = prep_features(df, symbols )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3itfJvAey1y"
      },
      "source": [
        "stock_win_df.sort_values(['symbol','date'])[-60:-30][['symbol','date','rolling_x','max_loss_30','y']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HeUNWNxCaAY"
      },
      "source": [
        "X = np.array(stock_win_df.rolling_x.values.tolist())\n",
        "y = np.array(stock_win_df.y.values.tolist())\n",
        "y = np.reshape(y, (y.shape[0],1))\n",
        "assert X.shape[0]== y.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaqPOaLSbBjF"
      },
      "source": [
        "def to_np(df):\n",
        "    X = np.asarray(df.rolling_x.values.tolist()).astype('float64')    \n",
        "    y = np.asarray(df.y.values.tolist()).astype('float64')\n",
        "    y = np.reshape(y, (y.shape[0], 1))    \n",
        "    return (X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Tf1Jxz0BlA"
      },
      "source": [
        "train_df = stock_win_df.loc[stock_win_df.date < \"2019-06-01\"]\n",
        "xval_df = stock_win_df.loc[(stock_win_df.date >='2019-06-01') & (stock_win_df.date < '2020-01-01')]\n",
        "test_df = stock_win_df.loc[stock_win_df.date > \"2020-01-01\"][:]  # we omit last 5 var because they are nan (we predict 5 steps in advance)\n",
        "train = to_np(train_df)\n",
        "xval = to_np(xval_df)\n",
        "test = to_np(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoB9nbV1TwlS"
      },
      "source": [
        "train[0].shape, train[1].shape, xval[0].shape, xval[1].shape, test[0].shape, test[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkn_5EKFU0B1"
      },
      "source": [
        "# QCNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12pm6gYbrLGU"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrz0GMitrE72"
      },
      "source": [
        "batch_size = 32\n",
        "train_len = train[0].shape[0]\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train)\n",
        "xval_ds = tf.data.Dataset.from_tensor_slices(xval)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test)\n",
        "\n",
        "train_ds = train_ds.shuffle(train_len).batch(batch_size)\n",
        "xval_ds = xval_ds.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSD4dWuoE4zY"
      },
      "source": [
        "def quantile_loss(q=0.5):\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        #print(q,y_true.shape, y_pred.shape)\n",
        "        e = (y_true - y_pred)        \n",
        "        return tf.keras.backend.mean(tf.keras.backend.maximum(q * e, (q - 1) * e),axis=-1)\n",
        "   \n",
        "    # Return a function\n",
        "    return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3lVpEhYrP_O"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(8, 2, activation='relu', padding='causal', input_shape=train[0].shape[1:]),\n",
        "    tf.keras.layers.Conv1D(8, 2, activation='relu', padding='causal', dilation_rate=2),\n",
        "    tf.keras.layers.Conv1D(8, 2, activation='relu', padding='causal', dilation_rate=4),\n",
        "    tf.keras.layers.Conv1D(8, 2, activation='relu', padding='causal', dilation_rate=8),\n",
        "    tf.keras.layers.Conv1D(8, 2, activation='relu', padding='causal', dilation_rate=16),\n",
        "    tf.keras.layers.Conv1D(8, 2, activation='relu', padding='causal', dilation_rate=32),\n",
        "    tf.keras.layers.Conv1D(1, 1, activation='linear'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optim,\n",
        "              loss= quantile_loss(0.05),\n",
        "              metrics=['mae', quantile_loss(0.05)])\n",
        "#                loss= tf.keras.losses.Huber() ,\n",
        "#                metrics=['mae', tf.keras.losses.Huber()])\n",
        "\n",
        "\n",
        "\n",
        "logdir = \"logs/var_preds/qcnn/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(histogram_freq=2, log_dir=logdir)\n",
        "early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "model.fit(train_ds,\n",
        "         epochs=128,\n",
        "         validation_data= xval_ds,\n",
        "         callbacks=[tensorboard_cb, early_stop_cb])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DqxZ6Tl--1u"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQdcGXMlQUP4"
      },
      "source": [
        "model.evaluate(test[0], test[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MjUy8pdB2pB"
      },
      "source": [
        "def prepare_stock_x(df, symbol, end_date, start_date):\n",
        "    win_df = prep_features(df, [symbol] )    \n",
        "    date_crit = (win_df.date<=end_date)&(win_df.date>=start_date)\n",
        "    data = to_np(win_df[date_crit])    \n",
        "    return data, win_df[date_crit].date.dt.date.values.tolist()\n",
        "     \n",
        "\n",
        "stock_period, dates = prepare_stock_x(df, 'FNDE', '2020-01-31','2019-01-01')\n",
        "x = stock_period[0]\n",
        "y = stock_period[1]\n",
        "y_pred = model.predict(x)\n",
        "preds = pd.DataFrame(list(zip(dates, y.squeeze(), y_pred.squeeze())), columns=['date','y','y_pred'])\n",
        "\n",
        "sns.lineplot(x='date', y='value', hue='variable', data=pd.melt(preds, ['date']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY785fFLCnyB"
      },
      "source": [
        "df.symbol.unique() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiH-IXjrQCf"
      },
      "source": [
        "%tensorboard --logdir logs/var_preds/qcnn\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}