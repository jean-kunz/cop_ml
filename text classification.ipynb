{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text classification.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ux4S8DC552xw","colab_type":"text"},"source":["# Text classification\n","\n","This notebook is about text classification and general principles of nlp.\n","\n","1. How to tokenize a text and get different representation of a text ready to ingest into a model. \n","\n","2. The concept of embeddings and how to make it in pure tf. \n","\n","3. A classifier using a classic DNN and a RNN (LSTM)\n","\n","We also check how to view embeddings in a 3d space in tensorboard"]},{"cell_type":"code","metadata":{"id":"pa9w-aF1sB9O","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PATDQcyCsRHr","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8inEx7OFSgl","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.text import Tokenizer, one_hot,text_to_word_sequence\n","from tensorflow.keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ysjd6lq56n2B","colab_type":"text"},"source":["### How to tokenize a text\n","\n","Get different representation of an input text using keras tokenizer.\n","\n","- as a matrix where each entry i,j is for the i sentence , the j token. Matrix cell contains the nb of occurence of each word in the sentence, a binary if present. It could also get a tfidf \n","- as one-hot encoded\n"]},{"cell_type":"code","metadata":{"id":"c6zs4hr3jU2f","colab_type":"code","colab":{}},"source":["import spacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oez6nNmwtbte","colab_type":"code","colab":{}},"source":["import spacy.cli\n","spacy.cli.download(\"en_core_web_md\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0v7cZFMjYW-","colab_type":"code","colab":{}},"source":["nlp = spacy.load(\"en_core_web_md\")\n","doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n","index_word = {}\n","word_index = {}\n","i =0\n","for token in doc:\n","    #print(token.text, token.pos_, token.lemma_, token.is_sent_start)\n","    word_index[token.text]=i\n","    index_word[i]=token.text\n","    i+=1\n","word_index, index_word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAZ2CjXom0tN","colab_type":"code","colab":{}},"source":["class SpacyTokenizer:\n","\n","    def __init__(self, num_words=None, do_lower=True, remove_stop_words = True, \n","                 replace_nb_with_fix_tok=False, oov_token='_OOV_', stop_words = []):\n","        self.num_words = num_words\n","        self.do_lower = do_lower\n","        self.remove_stop_words = remove_stop_words\n","        self.replace_nb_with_fix_tok = replace_nb_with_fix_tok\n","        self.index_word = {}\n","        self.word_index = {}\n","        self.word_count = {}\n","        self.oov_token = oov_token\n","        self.stop_words = stop_words\n","        self.use_spacy_stop_words = len(stop_words)==0\n","\n","    def __add_word(self, text, i): \n","        self.index_word[i]=text\n","        self.word_index[text]=i\n","        count = self.word_count.get(text,0)\n","        self.word_count[text] = count+1\n","\n","\n","    def __process_tok(self, tok):\n","        tok_text = None\n","        if self.do_lower:\n","            tok_text= tok.text.lower()\n","        else:\n","            tok_text = tok.text\n","\n","        if self.replace_nb_with_fix_tok:\n","            if tok.is_digit:\n","                tok_text = '__DIGIT__'\n","\n","        if self.remove_stop_words:\n","            if self.use_spacy_stop_words:\n","                if tok.is_stop:\n","                    tok_text = None\n","            else:\n","                if tok_text in self.stop_words:\n","                    tok_text = None\n","            \n","\n","        return tok_text\n","\n","    def fit_on_texts(self, texts):\n","        i = 0\n","        for txt in texts:\n","            doc = nlp (txt)\n","            for tok in doc:\n","                tok_text = self.__process_tok(tok)\n","                #print(tok_text)\n","                if tok_text is not None:\n","                    # word can already be in index. If true we don't add it, but reuse existing idx\n","                    idx = self.word_index.get(tok_text, i)\n","                    self.__add_word(tok_text,idx)\n","                    if (idx==i):\n","                        i+=1\n","        # eliminate words over num_words based on sort of count.\n","        top_words = [k for k,v in sorted(self.word_count.items(), key=lambda item: item[1], reverse=True)]\n","        top_words = top_words[:self.num_words]\n","\n","        self.top_words = {*top_words}\n","        self.__add_word(self.oov_token,i)\n","\n","                \n","    def texts_to_sequences(self, texts):\n","        seqs = []\n","        for txt in texts:\n","            doc = nlp(txt)\n","            seq = []\n","            for tok in doc:\n","                tok_text = self.__process_tok(tok)\n","                if tok_text is not None:\n","                    if tok_text in self.top_words:\n","                        tok_idx = self.word_index[tok_text]\n","                    else:\n","                        tok_idx = self.word_index[self.oov_token]\n","                    seq.append(tok_idx)\n","            seqs.append(seq)\n","        return seqs\n","                \n","\n","\n","sp_tok = SpacyTokenizer(num_words = 8, do_lower=True, replace_nb_with_fix_tok=True, oov_token='__OOV__', stop_words=['a','the','my'])\n","texts = [\"Apple is looking at buying U.K. startup for $1 billion.\", \"One should not eat the apple every day!  this is my 12 billion advice\"]\n","sp_tok.fit_on_texts(texts)\n","            \n","seq = sp_tok.texts_to_sequences(texts)\n","#ÃŸseq, sp_tok.top_words, sp_tok.word_index, sp_tok.index_word, sp_tok.word_count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vm-Jp8OvufvP","colab_type":"code","colab":{}},"source":["queen = nlp('queen')\n","queen_vect = queen.vector\n","king = nlp('king')\n","king_vect = king.vector\n","\n","man = nlp('man')\n","man_vect = man.vector\n","woman = nlp('woman')\n","woman_vect = woman.vector\n","\n","king_vect - man_vect + woman_vect - queen_vect\n","king_vect.argmax(), queen_vect.argmax()\n","man_vect.argmax(), woman_vect.argmax()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdbrBjg6jYbh","colab_type":"code","colab":{}},"source":["doc = nlp(\"I loved coffee\")\n","for word in doc:\n","    lexeme = doc.vocab[word.text]\n","    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n","            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_, word.lemma_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k7DQdpwRJl3f","colab_type":"code","colab":{}},"source":["vocab_nb=8\n","test_t = Tokenizer(num_words=vocab_nb,filters='',lower=True, oov_token='UNK' )\n","tmp_text = ['hello the people of the world .','You are the best in the world !']\n","tmp_text = [\"Apple is looking at buying U.K. startup for $1 billion.\", \"One should not eat apple every day!  this is my 12 billion advice\"]\n","\n","test_t.fit_on_texts(tmp_text)\n","#test_t.index_word, test_t.word_index, test_t.word_counts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUhNfykTFWfa","colab_type":"code","colab":{}},"source":["count_mat = test_t.texts_to_matrix(tmp_text, mode='count')\n","bin_mat = test_t.texts_to_matrix(tmp_text,mode='binary')\n","tfidf_mat = test_t.texts_to_matrix(tmp_text, mode='tfidf')\n","# tfidf set a low score for the, because it's frequent in each sentence > less meaningfull than a words that occurs in less sents. \n","# matrix shape takes num_words into account even if it has more words into dictionnary\n","count_mat, bin_mat, tfidf_mat,count_mat.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhD7M3_dHLX9","colab_type":"code","colab":{}},"source":["seq_len = 20\n","seq = test_t.texts_to_sequences(tmp_text)\n","padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq,maxlen=seq_len,padding='post')\n","seq,padded_seq, type(seq)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x68bnttgmU9g","colab_type":"text"},"source":["We one_hot encode the padded sequence. It creates a tensor of shape\n","(nb_samples, seq_len, vocab_nb)"]},{"cell_type":"code","metadata":{"id":"7SG98HEgi3zZ","colab_type":"code","colab":{}},"source":["seq_oh = tf.keras.backend.one_hot(tf.Variable(padded_seq),10)\n","seq_oh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-n1Rwt70vLW","colab_type":"text"},"source":["## Embeddings concept\n","\n","Let's create our own embedding layer and compare results with Keras one.\n","\n","The goal is to create a layer that extract the embeddings of a given sequence from a embedding matrix that contains the embeddings of all of the tokens/words.\n","\n","We extract embeddings from a random uniform matrix. In a learning problem, we will learn those embeddings with gradient descent. "]},{"cell_type":"code","metadata":{"id":"x4naZg1LnRQB","colab_type":"code","colab":{}},"source":["class Embeddings(object):\n","\n","    def __init__(self,vocab_size, embed_size):\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        init= tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=23)\n","\n","        self.embed_matrix = tf.Variable(init((vocab_size,embed_size)),dtype=tf.float32)\n","        print(\"embedding matrix shape:\",self.embed_matrix.shape)\n","        print(\"embedding matrix\", self.embed_matrix)\n","\n","    def forward(self,X):\n","        X_oh = tf.keras.backend.one_hot(X,self.vocab_size)\n","        print(X_oh)\n","        return tf.matmul(X_oh, self.embed_matrix,transpose_a=False,transpose_b=False)\n","        \n","\n","input = np.array([[1,2,3,0,0],[0,1,2,0,3]])\n","my_emb_layer = Embeddings(4,3)\n","my_emb = my_emb_layer.forward(input)\n","my_emb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Kdw-JhD0t8s","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c4NRbDBUf3n9","colab_type":"code","colab":{}},"source":["# Try keras version\n","k_emb_layer = Embedding(4,3,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=23))\n","k_emb = k_emb_layer(input)\n","\n","# Check results are the same between my embeddings and keras one. \n","tf.math.equal(k_emb,my_emb)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oa97AKBBf3rR","colab_type":"code","colab":{}},"source":["# Check that init of embedding_matrix are the same\n","tf.math.equal(k_emb_layer.embeddings,my_emb_layer.embed_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nAqOUR6pwhA1","colab_type":"text"},"source":["## Real case"]},{"cell_type":"code","metadata":{"id":"N3KUxFvytA_C","colab_type":"code","colab":{}},"source":["%%bash\n","[[ -d /content/.kaggle ]] || mkdir /content/.kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKzthoTos6RL","colab_type":"code","colab":{}},"source":["# get the keys from your kaggle account information\n","# my account > API > Create New API Token\n","# get the content of the file \n","import json\n","token = {\"username\":\"xxx\",\"key\":\"xxxxx\"}\n","with open('/content/.kaggle/kaggle.json', 'w') as file:\n","    json.dump(token, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpXQl-MAoe9Q","colab_type":"code","colab":{}},"source":["!ls /content/.kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zls7qxlBBE4Z","colab_type":"code","colab":{}},"source":["%%bash\n","\n","if [ ! -f /content/spam-text-message-classification.zip ]; then\n","    mkdir /root/.kaggle\n","    cp /content/.kaggle/kaggle.json /root/.kaggle/\n","    cat /root/.kaggle/kaggle.json\n","    kaggle config set -n path -v{/content}\n","    \n","    kaggle datasets download -d team-ai/spam-text-message-classification -p /content\n","    \n","    unzip -o /content/spam-text-message-classification.zip -d /content\n","fi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-v0mLTpB-2dv","colab_type":"code","colab":{}},"source":["%load_ext google.colab.data_table"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BV0LAHz9E-JR","colab_type":"code","colab":{}},"source":["data_df = pd.read_csv('/content/SPAM text message 20170820 - Data.csv')\n","data_df = data_df.sample(frac=1.,random_state=34).reset_index(drop=True)\n","data_df['labels'] = data_df.Category.astype('category').cat.codes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgG0yAA7--XD","colab_type":"code","colab":{}},"source":["data_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4ZFHles73Z6","colab_type":"code","colab":{}},"source":["nb_1 = data_df.loc[data_df.labels==1].labels.count()\n","nb_0=data_df.loc[data_df.labels==0].labels.count()\n","print(f\"CAUTION : majority class baseline will provide {nb_0 / (nb_1+nb_0)} accuracy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8wlPp7oLWwP","colab_type":"code","colab":{}},"source":["data_nb = data_df.labels.count()\n","train_nb = np.int(np.floor(data_nb *0.8))\n","test_nb = data_nb-train_nb\n","xval_nb = np.int(np.floor(train_nb*0.2))\n","train_nb = train_nb-xval_nb\n","assert data_nb == train_nb + xval_nb + test_nb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhdwxf56L4m-","colab_type":"code","colab":{}},"source":["labels = data_df.labels.values\n","text = data_df.Message.values\n","\n","train_labels = labels[:train_nb]\n","train_text = text[:train_nb]\n","\n","xval_labels = labels[train_nb:train_nb+xval_nb]\n","xval_text = text[train_nb:train_nb+xval_nb]\n","\n","test_labels = labels [-test_nb:]\n","test_text = text[-test_nb:]\n","# check there is no overlap\n","assert train_text[-1] != xval_text[0] and xval_text[-1]!=test_text[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2E9igjGATMN","colab_type":"code","colab":{}},"source":["train_text[:10], train_labels[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jlYQyFd6gos","colab_type":"code","colab":{}},"source":["use_spacy_tok = True\n","\n","vocab_size = 10000\n","if use_spacy_tok:\n","    sp_tok = SpacyTokenizer(num_words = vocab_size, do_lower=True, replace_nb_with_fix_tok=True, oov_token='__OOV__')\n","else:\n","    t = Tokenizer(num_words=vocab_size,filters='',oov_token='UNK',lower=True)\n","#t = Tokenizer(filters='',oov_token='UNK',lower=True)\n","t.fit_on_texts(train_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m0CfgsVX5Llv","colab_type":"code","colab":{}},"source":["len(t.index_word),t.index_word[10000],t.word_index['UNK'],t.num_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jbHmbkq1_ZYo","colab_type":"text"},"source":["### Export words for tensorboard projector\n","\n","Save words associated to each embeddings in meta.tsv to be able to view them in projector. "]},{"cell_type":"code","metadata":{"id":"q1P5yGjn1LzO","colab_type":"code","colab":{}},"source":["import io\n","\n","meta_file = io.open('meta.tsv', 'w', encoding='utf-8')\n","\n","ms = []\n","i=0\n","\n","for key in t.index_word:\n","    #print(t.index_word[key])\n","    if i< vocab_size:\n","        word = t.index_word[key]\n","        word = word.replace('\\n',' ')\n","        ms.append(word)    \n","        meta_file.write(word + \"\\n\")\n","    else:\n","        print(i)\n","        break\n","    i+=1\n","\n","meta_file.close()\n","\n","\n","# uncomment to download meta.tsv and then upload to tensorboard projector.\n","\n","try:\n","  from google.colab import files\n","except ImportError:\n","   pass\n","else:\n","  files.download('meta.tsv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyUX44139VAt","colab_type":"code","colab":{}},"source":["train_seq = t.texts_to_sequences(train_text)\n","xval_seq = t.texts_to_sequences(xval_text)\n","test_seq = t.texts_to_sequences(test_text)\n","len(train_seq)\n","train_seq[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-PkZuEvCUfV","colab_type":"code","colab":{}},"source":["train_seq[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RsqivFWuCp6w","colab_type":"code","colab":{}},"source":["t.index_word[10272]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_VXEtfe95DI","colab_type":"code","colab":{}},"source":["max_len=500\n","train_seq_pad = tf.keras.preprocessing.sequence.pad_sequences(train_seq,maxlen=max_len,padding='post') \n","xval_seq_pad = tf.keras.preprocessing.sequence.pad_sequences(xval_seq,maxlen=max_len,padding='post')\n","test_seq_pad = tf.keras.preprocessing.sequence.pad_sequences(test_seq,maxlen=max_len,padding='post')\n","\n","train_ds = tf.data.Dataset.from_tensor_slices((train_seq_pad,train_labels)).shuffle(train_seq_pad.shape[0])\n","xval_ds = tf.data.Dataset.from_tensor_slices((xval_seq_pad,xval_labels)).shuffle(xval_seq_pad.shape[0])\n","test_ds = tf.data.Dataset.from_tensor_slices((test_seq_pad,test_labels)).shuffle(test_seq_pad.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mp_fwxHIsGrX","colab_type":"code","colab":{}},"source":["\"data:\",train_seq_pad.shape, xval_seq_pad.shape, test_seq_pad.shape, \\\n","\"labels:\",train_labels.shape,xval_labels.shape,test_labels.shape, \\\n","vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J93D-cpfnsDz","colab_type":"code","colab":{}},"source":["tf.keras.backend.clear_session()\n","embedding_dim=300\n","\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Embedding(vocab_size, embedding_dim),\n","  # we average along each word embedding of the sequence, so we are agnostic to seq.len. \n","  tf.keras.layers.GlobalAveragePooling1D(),\n","  tf.keras.layers.Dense(64, activation='relu'),\n","  tf.keras.layers.Dense(32, activation='relu'),\n","  tf.keras.layers.Dense(1,activation='sigmoid')\n","])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVJoDwvTo5On","colab_type":"code","colab":{}},"source":["import datetime\n","\n","model.compile(optimizer='adam',\n","             loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n","              metrics=['accuracy'])\n","\n","train_batch_ds = train_ds.batch(64)\n","xval_batch_ds = xval_ds.batch(64)\n","\n","log_dir=\"logs/dnn-fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True,\n","                                                      embeddings_freq=1)\n","\n","\n","history = model.fit(\n","    train_batch_ds,\n","    validation_data=xval_batch_ds,\n","    callbacks=[tensorboard_cb],\n","    epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZP8Q4uD-fjQ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHQaqoI0EsLu","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdNUkGuqr6y3","colab_type":"code","colab":{}},"source":["e = model.layers[0]\n","weights = e.get_weights()[0]\n","print(weights.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gsOzOi2W8R5Z","colab_type":"text"},"source":["### Apply model on test data"]},{"cell_type":"code","metadata":{"id":"SgtALT8c8Rfm","colab_type":"code","colab":{}},"source":["test_loss, test_acc = model.evaluate(test_ds.batch(100))\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1EiMDt8l8q0D","colab_type":"code","colab":{}},"source":["def np_sent_to_str(np_sent):\n","    toks = [t.index_word[tok] for tok in np_sent if tok >0]\n","    return \" \".join(toks)\n","\n","def np_sentence_to_str_sentence(np_sents):\n","    str_sents = []\n","    for sent in np_sents:\n","        str_sents.append(np_sent_to_str(sent))        \n","    return str_sents\n","\n","\n","def get_test_preds(model, test_ds):\n","    \n","    iter = test_ds.as_numpy_iterator()\n","    test_seq = next(iter)\n","    test_seq_np = test_seq[0]\n","    test_label = test_seq[1]\n","\n","    test_label_pred = model.predict_classes(test_seq)\n","\n","    test_sents = np_sentence_to_str_sentence(test_seq_np)\n","    \n","\n","    return pd.DataFrame(np.column_stack((test_sents,test_label,test_label_pred)),columns=['sentence','ground_truth','pred'])\n","\n","get_test_preds(model,test_ds.batch(20))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOSG-cM7zS0O","colab_type":"code","colab":{}},"source":["t.word_index['hello']\n","t.index_word[7]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWyld0x5wZLQ","colab_type":"code","colab":{}},"source":["def str_sents_to_np(str_sent,max_len=20):\n","    seq = t.texts_to_sequences([str_sent])\n","    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq,maxlen=max_len,padding='post')\n","\n","    return padded_seq\n","\n","def pred_sent(model, str_sent, max_len=20):\n","    print(max_len)\n","    str_sents = [str_sent]\n","    seq = str_sents_to_np(str_sent,max_len=max_len)\n","    return model.predict(seq)\n","\n","\n","str_sent=\"Now the new samsung s5 for free !\"\n","pred_sent(model,str_sent, max_len=max_len)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tsnv1mxOD0N","colab_type":"text"},"source":["## RNN model\n"]},{"cell_type":"code","metadata":{"id":"QNA7Su7EOCz9","colab_type":"code","colab":{}},"source":["rnn_model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, 64, mask_zero=True),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64,  return_sequences=True)),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1,activation='sigmoid')\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FoCxiKttBsKk","colab_type":"code","colab":{}},"source":["rnn_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oh_6onut7Rnt","colab_type":"code","colab":{}},"source":["train_batch_ds = train_ds.batch(32)\n","xval_batch_ds = xval_ds.batch(32)\n","\n","log_dir=\"logs/rnn-fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True,\n","                                                      embeddings_freq=1)\n","\n","history = rnn_model.fit(train_batch_ds, epochs=10,\n","                    validation_data=xval_batch_ds,\n","                    callbacks=[tensorboard_cb], \n","                    validation_steps=5)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7WQNjRSPCl9x","colab_type":"code","colab":{}},"source":["# On test data\n","test_loss, test_acc = rnn_model.evaluate(test_ds.batch(100))\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_akERIE2I3u","colab_type":"code","colab":{}},"source":["get_test_preds(rnn_model,test_ds.batch(20))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9RozJ_mX9y3k","colab_type":"code","colab":{}},"source":["str_sent=\"Now get the new samsung s5 for 499 USD !\"\n","pred_sent(rnn_model,str_sent, max_len=max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F8okVMu-EBv5","colab_type":"code","colab":{}},"source":["rnn_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KlqFyym35gBo","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfuEU-bFCRdZ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}