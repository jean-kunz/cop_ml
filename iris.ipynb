{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "iris.ipynb",
   "provenance": [],
   "private_outputs": true,
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python38364bitmlconda85204436fb7b4054a133331e01721531",
   "language": "python",
   "display_name": "Python 3.8.3 64-bit ('ml': conda)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZZ8108ULBsaV",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%tensorflow_version 2.x"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LJHtnI1cByBf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-739359f3b980>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B7j8FGHFS0a9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGDxFaa5HEmi",
    "colab_type": "text"
   },
   "source": [
    "## Iris dataset\n",
    "\n",
    "- turn it into 32bit float\n",
    "- split in train and test dataset\n",
    "- one hot encode target (y)\n",
    "- create a tf.dataset for train and test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GlJVOjpQC3CA",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "iris = datasets.load_iris()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-f811bec3b759>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0miris\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_iris\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'datasets' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jJIJGAPtC9rE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# convert to float32\n",
    "data = iris.data\n",
    "data = data.astype(np.float32)\n",
    "labels = iris.target\n",
    "classes_nb = np.max(labels)+1  #3"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-023504379089>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# convert to float32\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0miris\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0miris\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mclasses_nb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m  \u001B[0;31m#3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'iris' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e5ni0QvR8Dpp",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "iris.target, iris.target.shape"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-aabddcb8466f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0miris\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miris\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'iris' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G5NXQRDWDTKi",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# split in train and test\n",
    "y_oh_train = tf.one_hot(y_train, len(np.unique(iris.target)), dtype=tf.float32)\n",
    "\n",
    "train_nb = X_train.shape[0]\n",
    "test_nb = X_test.shape[0]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "noWJKIJ7dLol",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# create train and test datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jpdku83feaJx",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# iterate over 1 sample to understand its content\n",
    "# it return a tuple of tensors\n",
    "next(iter(train_ds))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6SudmEbJBZ8",
    "colab_type": "text"
   },
   "source": [
    "# Model\n",
    "\n",
    "We create a pure tensorfow model (without using any keras facilities like layers). We only use losses, initializers from Keras.\n",
    "\n",
    "We setup : \n",
    "- layer structure\n",
    "- a loss function\n",
    "- a train_on_mini_batch function\n",
    "- a training loop of multiple epochs on many batches\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NfuQaKMeJYrZ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# create train and test datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "def get_glorot_uniform_tensor(shape):\n",
    "    return tf.keras.initializers.GlorotUniform(seed=42)(shape)\n",
    "\n",
    "def get_normal_tensor(shape):\n",
    "    return tf.random.normal(shape,mean=0.0, stddev=0.05)\n",
    "\n",
    "def get_zero_tensor(shape):\n",
    "    return tf.zeros(shape)\n",
    "\n",
    "get_glorot_uniform_tensor((3,2)).numpy(), get_normal_tensor((3,2)).numpy(), get_zero_tensor((3,2)).numpy()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLphNGUm6_As",
    "colab_type": "text"
   },
   "source": [
    "![Model propos√©](https://www.tensorflow.org/images/custom_estimators/full_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "clxqBLCeVWFF",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        w_init = get_glorot_uniform_tensor # how we initialize our weights        \n",
    "        b_init = get_zero_tensor # bias init.\n",
    "\n",
    "        self.W1 = tf.Variable(w_init((4,10)), dtype=tf.float32)\n",
    "        self.b1 = tf.Variable(b_init((10)), dtype=tf.float32)\n",
    "\n",
    "        self.W2 = tf.Variable(w_init((10,10)), dtype=tf.float32)\n",
    "        self.b2 = tf.Variable(b_init((10)), dtype=tf.float32)\n",
    "\n",
    "        self.W3 = tf.Variable(w_init((10,3)), dtype=tf.float32)\n",
    "        self.b3 = tf.Variable(b_init((3)), dtype=tf.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        Z1 = tf.add(tf.matmul(x, self.W1), self.b1)\n",
    "        A1 = tf.nn.relu(Z1)\n",
    "\n",
    "        Z2 = tf.add(tf.matmul(A1, self.W2), self.b2)\n",
    "        A2 = tf.nn.relu(Z2)\n",
    "\n",
    "        Z3 = tf.add(tf.matmul(A2, self.W3), self.b3)\n",
    "        A3 = tf.nn.softmax(Z3)\n",
    "\n",
    "        return A3\n",
    "\n",
    "model = Model()\n",
    "pred_train = model.forward(tf.constant(X_train,dtype=tf.float32))\n",
    "#tf.reduce_sum(pred_train,axis=1)           "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w72j0T1kIx0b",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def compute_loss(y_pred,y_true):\n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    loss= cce(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "compute_loss(pred_train, y_train)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H4ew08jWPFJ6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Home made optimizer using tensorflow automatic computation of gradients\n",
    "def train_minibatch(model, X, y_true, learning_rate=0.01):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss =  compute_loss(model.forward(X), y_true)\n",
    "    \n",
    "    #print(model.b1)\n",
    "    dW1, db1, dW2, db2, dW3, db3= t.gradient(current_loss, [model.W1, model.b1, model.W2, model.b2, model.W3, model.b3])\n",
    "    #print(db1)\n",
    "    model.W1.assign_sub(learning_rate * dW1)\n",
    "    model.b1.assign_sub(learning_rate * db1)\n",
    "    #print(model.b1)\n",
    "    model.W2.assign_sub(learning_rate * dW2)\n",
    "    model.b2.assign_sub(learning_rate * db2)\n",
    "    model.W3.assign_sub(learning_rate * dW3)\n",
    "    model.b3.assign_sub(learning_rate * db3)\n",
    "    return current_loss\n",
    "\n",
    "train_minibatch(model,\n",
    "                next(iter(train_ds.batch(10)))[0], #X\n",
    "                next(iter(train_ds.batch(10)))[1], #y_true\n",
    "                0.5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wz60dknQTZuY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# https://datascience.stackexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent\n",
    "\n",
    "# use keras optimizer to enhance gradient descent performances (finds optimal ways to update parameters)\n",
    "\n",
    "def train_minibatch_with_k_optim(model,X,y_true,learning_rate=0.01):\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate) # the SGD optimizer!\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss =  compute_loss(model.forward(X), y_true)\n",
    "\n",
    "    # compute gradients\n",
    "    dW1, db1, dW2,db2,dW3, db3 = t.gradient(current_loss, [model.W1, model.b1,model.W2, model.b2,model.W3,model.b3])\n",
    "\n",
    "    # apply gradient descent using SGD optimizer\n",
    "    optimizer.apply_gradients(zip([dW1, db1, dW2,db2,dW3, db3], [model.W1, model.b1,model.W2, model.b2,model.W3,model.b3]))\n",
    "\n",
    "    return current_loss\n",
    "    "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SGkTJu2ILWrH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Collect the history of W-values and b-values to plot later\n",
    "W1s, b1s,W2s,b2s,W3s,b3s = [], [],[],[],[],[]\n",
    "losses =[]\n",
    "accuracies = []\n",
    "epochs = range(100)\n",
    "batch_size = train_nb // 4\n",
    "cat_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = Model()\n",
    "\n",
    "for epoch in epochs:\n",
    "\n",
    "    W1s.append(model.W1.numpy())\n",
    "    b1s.append(model.b1.numpy())\n",
    "    W2s.append(model.W2.numpy())\n",
    "    b2s.append(model.b2.numpy())\n",
    "    W3s.append(model.W3.numpy())\n",
    "    b3s.append(model.b3.numpy())\n",
    "\n",
    "    # shuffle batch into minibatches using N=train_nb random samples\n",
    "    train_batch_ds = train_ds.shuffle(buffer_size=train_nb).batch(batch_size, drop_remainder=True) # use the tensorslicedataset's batch function\n",
    "    batch_nb = 0\n",
    "\n",
    "    # for each minibatch, compute and apply gradient descent \n",
    "    for x, y in train_batch_ds:\n",
    "        current_loss = train_minibatch(model, x, y, learning_rate=learning_rate) # current loss\n",
    "        batch_nb = batch_nb+1 # new batch\n",
    "\n",
    "    # store epoch's actual loss\n",
    "    losses.append(current_loss.numpy())\n",
    "\n",
    "    # create a new random batch of N=train_nb samples\n",
    "    X_train, y_train = next(iter(train_ds.batch(train_nb)))\n",
    "\n",
    "    # compute ouput label probabilities at epoch\n",
    "    y_pred_train = model.forward(X_train)\n",
    "\n",
    "    # compute epoch accuracy\n",
    "    acc = cat_acc(y_train,y_pred_train)\n",
    "    accuracies.append(acc.numpy())\n",
    "    if epoch % 10 == 0:\n",
    "        print(current_loss.numpy(),acc.numpy())"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8kKOkTAIgP3",
    "colab_type": "text"
   },
   "source": [
    "## Plot learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pccm6b62IfiN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.plot(accuracies)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(losses)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL51KUW5A7Re",
    "colab_type": "text"
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "16cIHJ3gA6cY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "X_test, y_test = next(iter(test_ds.batch(test_nb)))\n",
    "y_pred_test = model.forward(X_test)\n",
    "acc = cat_acc(y_test,y_pred_test)\n",
    "print(\"accuracy of test: \",acc.numpy())"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBhR2PPd_bwL",
    "colab_type": "text"
   },
   "source": [
    "## Pure Keras "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8jM5o7UqSnoO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "k_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42), activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42), activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(3,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42), activation=tf.nn.softmax  )\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "k_model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "history = k_model.fit(train_ds.shuffle(buffer_size=train_nb).batch(batch_size=batch_size), epochs=20,verbose=1)\n",
    "#history.history['accuracy'][-1], history.history['loss'][-1]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jJ9PIpRhHPYB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "X_test, y_test = next(iter(test_ds.batch(test_nb)))\n",
    "y_k_pred_test = k_model.predict(X_test)\n",
    "acc = cat_acc(y_test,y_k_pred_test)\n",
    "print(\"accuracy of test: \",acc.numpy())"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXelY4vnJHFN",
    "colab_type": "text"
   },
   "source": [
    "### Plot keras learning process"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lFQP_VVDHwfN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V0f2Zc8PJMR5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "tf.config.list_physical_devices()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Cl3qYXudGdGk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}