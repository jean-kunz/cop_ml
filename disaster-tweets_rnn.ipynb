{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install en_core_web_lg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport re\nimport os\nimport nltk\n\n#nltk.download('stopwords')\n#from nltk.tokenize import TweetTokenizer\n#from nltk.corpus import stopwords\n#from nltk.stem import PorterStemmer\nimport spacy\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_nlp = spacy.load(\"en_core_web_lg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df= pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df= pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample_sub_df= pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\ntrain_val_tweets = train_df.text.values.tolist()\ntest_tweets = test_df.text.values.tolist()\ntrain_val_labels = train_df.target.values\n\nfrom sklearn.model_selection import train_test_split\ntrain_tweets, val_tweets, y_train, y_val = train_test_split(train_val_tweets, train_val_labels, test_size=0.10, random_state=42, shuffle=True)\nnp.unique(y_train, return_counts=True)[1], np.unique(y_val, return_counts=True)[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Data preparation\n\n##  text cleaning","metadata":{}},{"cell_type":"code","source":"class MyTweetTokenizer:\n\n    def __init__(self, num_tokens=10000, do_lower=True, remove_stop_words = True, \n                 replace_nb_with_fix_tok=True, oov_token='_OOV_', stop_words = [], use_lemma=True, nlp_model = None):\n        self.num_tokens = num_tokens\n        self.do_lower = do_lower        \n        self.index_token = {0: '_PAD_', 1: oov_token}\n        self.token_index = {'_PAD_': 0, oov_token: 1}        \n        self.token_count = {'_PAD_':0, oov_token: 0}\n        self.oov_token = oov_token\n\n        self.replace_nb_with_fix_tok = replace_nb_with_fix_tok\n        #self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n        #self.stopwords_english = stopwords.words('english')\n        #self.stemmer = PorterStemmer()\n        self.DIGIT_TOKEN = '_DIGIT_'\n        self.URL_TOKEN = '_URL_'\n        self.EMAIL_TOKEN = '_EMAIL_'\n        self.use_lemma = use_lemma\n        if nlp_model==None:\n            self.nlp = spacy.load(\"en_core_web_sm\", disable=['tagger','parser','ner'])\n        else:\n            self.nlp = nlp_model\n        \n        self.remove_stop_words = remove_stop_words        \n        self.stop_words = stop_words\n        if len(stop_words)>0:\n            print(\"custom stop words\")\n            self.nlp.Defaults.stop_words=self.stop_words\n\n    def __add_token(self, token, i): \n        self.index_token[i]=token\n        self.token_index[token]=i\n        count = self.token_count.get(token,0)\n        self.token_count[token] = count+1\n        \n        \n    def __clean_emoji(self, tweet):        \n        # have many categories\n        tweet = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"\"]+\", flags=re.UNICODE).sub(r'__emoticon__', tweet)\n        tweet = re.compile(\"[\"u\"\\U0001F300-\\U0001F5FF\"\"]+\", flags=re.UNICODE).sub(r'__pictograph__', tweet)\n        tweet = re.compile(\"[\"u\"\\U0001F680-\\U0001F6FF\"\"]+\", flags=re.UNICODE).sub(r'__transport__', tweet)\n        tweet = re.compile(\"[\"u\"\\U0001F1E0-\\U0001F1FF\"\"]+\", flags=re.UNICODE).sub(r'__flag__', tweet)\n        tweet = re.compile(\"[\"u\"\\U00002702-\\U000027B0\"\n                              u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE).sub(r'__other_emoji__', tweet)\n        \n        '''\n        emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   \"]+\", flags=re.UNICODE)\n        tweet = emoji_pattern.sub(r'__emoji__', tweet)\n        '''        \n        return tweet\n        \n\n    def __process_tweet(self, tweet):   \n        # remove emoji -> we keep emoji cause they have meaning\n        tweet = self.__clean_emoji(tweet)\n        \n        if self.do_lower:\n            tweet = tweet.lower()\n        # remove old style retweet text \"RT\"\n        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n        # remove hashtags\n        # only removing the hash # sign from the word\n        tweet = re.sub(r'#', '', tweet)\n        # remove html\n        tweet = re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', tweet)        \n        # remove special chars \n        # repr to convert to raw string in order to handle this case\n        tweet = re.sub(r\"\\\\x\\d\\d\\w\\w\",'', repr(tweet))\n        \n        return tweet\n    \n    \n    def __process_token_type(self, token):\n        if token.is_digit:\n            return_token = self.DIGIT_TOKEN\n        elif token.like_url:                 \n            return_token = self.URL_TOKEN                \n        elif token.like_email:\n            return_token = self.EMAIL_TOKEN\n        else:\n            return_token = token.lemma_\n        return return_token\n    \n    def __process_token(self, token):\n        return_token = None     \n        \n        \n        if self.remove_stop_words:\n            if not token.is_stop:\n                return_token = self.__process_token_type(token)\n        else:\n            return_token = self.__process_token_type(token)\n                \n        return return_token\n        \n        \n        \n    def fit_on_texts(self, tweets):\n        \n        i = len(self.index_token)  # we start after already entered tokens\n        for tweet in tweets:\n            tweet = self.__process_tweet(tweet)\n            # tokenize tweets\n            #print(\"pre-tokenizer:\",tweet)\n            tweet_tokens = self.nlp(tweet) #self.tokenizer.tokenize(tweet)    \n            \n            for tok in tweet_tokens:\n                #print(\">\",tok)\n                tok = self.__process_token(tok)\n                if tok is not None:\n                    #print(\">\",tok, type(tok))\n                    # tok can already be in index. If true we don't add it, but reuse existing idx\n                    idx = self.token_index.get(tok, i)\n                    self.__add_token(tok,idx)\n                    if (idx==i):\n                        i+=1\n \n        ## set token counts of OOV and PAD to vocab size so they are keyps\n        # eliminate words over num_words based on sort of count.\n        top_tokens = [k for k,v in sorted(list(self.token_count.items()), key=lambda item: item[1], reverse=True)]\n        top_tokens = top_tokens[:self.num_tokens-2] \n        top_token_index = {'_PAD_': 0, self.oov_token: 1} \n        \n        self.token_index = {**top_token_index, \n                            **{t: i+2 for i, t in enumerate(top_tokens)}}\n        self.index_token = {v : k for (k, v) in self.token_index.items()}\n        self.top_tokens = {*top_tokens}        \n\n        \n    def __texts_to_sequences(self, tweets, with_token_counts=False):\n        counters = {}\n        seqs = []\n        for tweet in tweets:\n            tweet = self.__process_tweet(tweet)\n            tweet_tokens = self.nlp(tweet)\n            seq = []\n            for tok in tweet_tokens:\n                tok = self.__process_token(tok) \n                \n                if tok is not None:                    \n                    if tok in self.top_tokens:\n                        #print(\"inv\",tok)\n                        tok_idx = self.token_index[tok]\n                    else:                        \n                        tok_idx = self.token_index[self.oov_token]\n                    \n                    tok_count = counters.get(tok_idx,0)\n                    counters[tok_idx]=tok_count+1\n                    \n                    seq.append(tok_idx)\n            seqs.append(seq)\n        return seqs, counters\n    \n                \n    def texts_to_sequences(self, tweets):\n        \n        seqs, _ = self.__texts_to_sequences(tweets, with_token_counts=False)\n        \n        return seqs\n    \n    def texts_to_token_count(self, tweets):        \n        seqs, counters = self.__texts_to_sequences(tweets, with_token_counts=False)\n        return counters\n    \n    \n    def sequences_to_texts(self, seqs):\n        texts = []\n        for seq in seqs:\n            text = ' '.join([self.index_token.get(tok_idx,'') for tok_idx in seq])            \n            texts.append(text)\n        return texts\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sm_tt = MyTweetTokenizer(num_tokens=25, nlp_model= lg_nlp, use_lemma=True, remove_stop_words=False)\nsm_tweets = [\"the dog\",\n                \"this is a new 2000 @jk http://www.news.com \\U0001F600 !- johndoe@acme.org“) \",\n               \"#go https://www.ls.ch:80/home?lan=fr long life \\x89ÛÒ to sport in 2021 #football HOP LS not me !-)) ?!!!\"]\nsm_tt.fit_on_texts(sm_tweets)\n#seqs = \n#tt.texts_to_sequences(test_tweets)\n#tt.sequences_to_texts(seqs)\nsm_tt.token_index, sm_tt.index_token\n#tt.sequences_to_texts(tt.texts_to_sequences(test_tweets)), \nsm_tt.texts_to_sequences(sm_tweets), sm_tt.token_index['_OOV_'], sm_tt.texts_to_token_count(sm_tweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer for train/test\nVOCAB_SIZE = 30000\ntweet_tok = MyTweetTokenizer(VOCAB_SIZE, remove_stop_words=False)\ntweet_tok.fit_on_texts(train_tweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check that tweet with straing escape char (like \\x89ÛÒ) are cleaned correctly\nprint(train_df[train_df.id==56].text.values.tolist())\ntoks = tweet_tok.texts_to_sequences(train_df[train_df.id==56].text.values.tolist())\ntweet_tok.sequences_to_texts(toks)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_seq = tweet_tok.texts_to_sequences(train_tweets)\nval_seq = tweet_tok.texts_to_sequences(val_tweets)\ntest_seq = tweet_tok.texts_to_sequences(test_tweets)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len of 40 captures most of tweets\nSEQ_LEN=40\ntrain_seq_pad = tf.keras.preprocessing.sequence.pad_sequences(train_seq,maxlen=SEQ_LEN,padding='post') \nval_seq_pad = tf.keras.preprocessing.sequence.pad_sequences(val_seq,maxlen=SEQ_LEN,padding='post')\ntest_seq_pad = tf.keras.preprocessing.sequence.pad_sequences(test_seq,maxlen=SEQ_LEN,padding='post')\n\nX_test = test_seq_pad\nX_val = val_seq_pad\nX_train = train_seq_pad\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tok_counts = tweet_tok.texts_to_token_count(test_tweets)\nval_tok_counts = tweet_tok.texts_to_token_count(val_tweets)\ntrain_tok_counts = tweet_tok.texts_to_token_count(train_tweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tok_counts\noov_tok_idx = tweet_tok.token_index['_OOV_']\ntweet_tok.index_token[oov_tok_idx]\n\n\"train oov\", train_tok_counts.get(oov_tok_idx),\"val oov\", val_tok_counts.get(oov_tok_idx), \"test oov\", test_tok_counts.get(oov_tok_idx)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compute tweet len","metadata":{}},{"cell_type":"code","source":"train_max_len = max([len(tweet) for tweet in train_seq])\ntrain_max_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(np.array([len(tweet) for tweet in train_seq]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split train into train and xval","metadata":{}},{"cell_type":"code","source":"train_df[train_df.target==1].target.count(), train_df[train_df.target==0].target.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(X_train.shape[0])\nval_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\ntest_ds = tf.data.Dataset.from_tensor_slices((test_seq_pad))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\ntf.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### compute initial bias ","metadata":{}},{"cell_type":"code","source":"neg, pos = np.bincount(train_df.target)\ntotal = neg + pos\ninitial_bias = np.log([pos/neg])\ninitial_bias, pos/neg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"THRESHOLD=0.5\n# use f1 for evaluation\nMETRICS = [        \n    tfa.metrics.F1Score(num_classes=2, average='micro', threshold=THRESHOLD)    \n]\n\ndef make_rnn_model(seq_len, vocab_size, hidden_size, nb_recurrent_layers=1, is_bidirectional=True, metrics=None, output_bias=None, lr=1e-3, do_dropout=False, do_batch_norm=False):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n\n    if do_dropout==True:        \n        dropout=0.5\n    else:\n        dropout=0.\n    inputs = tf.keras.layers.Input(shape=(seq_len,))    # use mask_zero=True for variable sequence len\n    x = tf.keras.layers.Embedding(vocab_size, hidden_size, mask_zero=True)(inputs) \n    \n    for i in range(nb_recurrent_layers): \n        if i<nb_recurrent_layers-1:\n            do_return_seq = True\n        else:\n            do_return_seq = False\n        if is_bidirectional:\n            x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_size, dropout=dropout,recurrent_dropout=dropout, return_sequences=do_return_seq))(x)\n        else:\n            x = tf.keras.layers.GRU(hidden_size, dropout=dropout,recurrent_dropout=dropout, return_sequences=do_return_seq)(x)\n        if do_batch_norm:\n            x = tf.keras.layers.BatchNormalization()(x)    \n    \n    x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n    if do_dropout:\n        x = tf.keras.layers.Dropout(dropout)(x)\n    if do_batch_norm:\n        x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Dense(hidden_size/2, activation='relu')(x)\n    if do_dropout:\n        x = tf.keras.layers.Dropout(dropout)(x)\n    if do_batch_norm:\n        x = tf.keras.layers.BatchNormalization()(x)\n        \n    outputs= tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)(x)\n    \n    #x=  tf.keras.layers.GRU(hidden_size, )(x)\n    #x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n    ## init bias in final layer correctly. We initialize the bias to our prior knowledge \n    #outputs = tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"rnn\")    \n        \n    #0.0005\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr),\n                  loss=tf.keras.losses.BinaryCrossentropy(),\n                  metrics=metrics)\n    return model\n\nmake_rnn_model(SEQ_LEN, VOCAB_SIZE, 64, nb_recurrent_layers=1, is_bidirectional=True, metrics=METRICS, output_bias=initial_bias).summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ensure model fit to a small dataset\n\ntry with a dataset of 2 and 10. It should perfecBinaryAccuracyBatchNormalization","metadata":{}},{"cell_type":"code","source":"do_small_data_fit = True\n\nif do_small_data_fit:\n    def create_small_data(X,y, nb_per_class=1):\n        true_idxs = np.where(y == True)[0][:nb_per_class]\n        false_idxs = np.where(y == False)[0][:nb_per_class]\n\n        idxs = np.concatenate((true_idxs, false_idxs)).tolist()    \n        X_sm = X[idxs]\n        y_sm = y[idxs]\n        return X_sm, y_sm\n\n    # try with 2 and 10 samples\n    SM_NB=10\n    X_train_sm, y_train_sm = create_small_data(X_train, y_train, nb_per_class=int(SM_NB/2))\n\n\n    sm_model = make_rnn_model(SEQ_LEN, VOCAB_SIZE, 64, nb_recurrent_layers=2, is_bidirectional=False, do_dropout=False, do_batch_norm=False, metrics=METRICS, output_bias=initial_bias)\n    sm_history = sm_model.fit(\n        X_train_sm,\n        y_train_sm,\n        batch_size=SM_NB,\n        epochs=100,\n        verbose=0)\n\n    threshold = 0.5\n    sm_pred_prob = sm_model.predict(X_train_sm)\n    sm_pred = np.where(sm_pred_prob > THRESHOLD, 1,0)\n    print(\"pred probabilities: \")\n    print(sm_pred_prob)\n    print(\"pred:\")\n    print(sm_pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mpl.rcParams['figure.figsize'] = (8, 6)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\ndef plot_metric(history, label, metric, n):\n# Use a log scale on y-axis to show the wide range of values.\n  plt.plot(history.epoch, history.history[metric],\n               color=colors[n], label='Train ' + label)\n  plt.plot(history.epoch, history.history['val_'+metric],\n               color=colors[n], label='Val ' + label,\n               linestyle=\"--\")\n  plt.legend()\n  plt.xlabel('Epoch')\n  plt.ylabel(metric)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.list_physical_devices('GPU')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train and test model","metadata":{}},{"cell_type":"code","source":"import datetime\nEPOCHS = 10\nBATCH_SIZE = 64\ndo_regularize = True\nif do_regularize:\n    cbs = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, verbose=1),\n          tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.001,verbose=1)]\nelse:\n    cbs = [tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.001,verbose=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_model = make_rnn_model(SEQ_LEN, VOCAB_SIZE, 64,  nb_recurrent_layers=1 , is_bidirectional=False, metrics=METRICS, output_bias=initial_bias, lr=3e-4, do_dropout=True, do_batch_norm=True)\nrnn_history = rnn_model.fit(\n        X_train,\n        y_train,\n        batch_size=BATCH_SIZE,\n        epochs=20,\n        validation_data=(X_val, y_val),\n        callbacks=cbs,\n        verbose=1)\n    \nplot_metric(rnn_history, \"RNN\",'loss', 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### log_dir=\"logs/rnn-model/\" + datetime.datetime.now().strftime(\"%Y.%m.%d-%H.%M\")\n#tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, embeddings_freq=1)\n#cbs.append(tensorboard_cb)\n\nrnn_model = make_rnn_model(SEQ_LEN, VOCAB_SIZE, 64,  nb_recurrent_layers=2 , is_bidirectional=False, metrics=METRICS, output_bias=initial_bias, lr=3e-4, do_dropout=True, do_batch_norm=True)\nrnn_history = rnn_model.fit(\n        X_train,\n        y_train,\n        batch_size=BATCH_SIZE,\n        epochs=20,\n        validation_data=(X_val, y_val),\n        callbacks=cbs,\n        verbose=1)\n    \nplot_metric(rnn_history, \"RNN\",'loss', 0)\n\nplot_metric(rnn_history, \"RNN\",'f1_score', 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check if lot of oov in val_X\nVOCAB_SIZE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model evaluation\n\n| Model | f1 train | f1 val | Status |\n| :-----|:--------:|:------:|:------:|\n| seq_len=40, vocab=20k, hidden=32, nb_rec_layers=1, bidi=True, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0.89 | 0.74 | ok |\n| seq_len=40, vocab=20k,  hidden=32, nb_rec_layers=2, bidi=True, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0.89 | 0.75 | ok |\n| seq_len=40, vocab=20k,  hidden=16, nb_rec_layers=1, bidi=True, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0.74 | 0.68 | ok |\n| seq_len=40, vocab=20k,  hidden=16, nb_rec_layers=2, bidi=True, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0.51 | 0.69 | ok |\n| seq_len=40, vocab=20k,  hidden=64, nb_rec_layers=1, bidi=True, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0.94 | 0.75 | ok |\n| seq_len=40, vocab=20k,  hidden=64, nb_rec_layers=2, bidi=True, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0.96 | 0.74 | ok |\n| seq_len=40, vocab=20k, hidden=32, nb_rec_layers=1,  bidi=False, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0. | 0. |  |\n| seq_len=40, vocab=20k,  hidden=32, nb_rec_layers=2, bidi=False, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0. | 0. |  |\n| seq_len=40, vocab=20k,  hidden=16, nb_rec_layers=1, bidi=False, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0. | 0. |  |\n| seq_len=40, vocab=20k,  hidden=16, nb_rec_layers=2, bidi=False, drop=True, batch_norm=True, epoch=20, lr=3e-4 | 0. | 0. |  |\n| seq_len=40, vocab=20k,  hidden=64, nb_rec_layers=1, bidi=False, drop=True, batch_norm=True, epoch=20, lr=3e-4 |0.95. | 0.7297 | ok |\n| seq_len=40, vocab=20k,  hidden=64, nb_rec_layers=2, bidi=False, drop=True, batch_norm=True, epoch=20, lr=3e-4 |0.9410| 0.7280 | ok |\n| seq_len=40, vocab=30k,  hidden=64, nb_rec_layers=1, bidi=False, drop=True, batch_norm=True, epoch=20, lr=3e-4 |0.9491 | 0.7230 | ok |\n| seq_len=40, vocab=30k,  hidden=64, nb_rec_layers=2, bidi=False, drop=True, batch_norm=True, epoch=20, lr=3e-4 |0.9427| 0.7171 | ok |\n\n","metadata":{}},{"cell_type":"code","source":"test_pred_prob = rnn_model.predict(X_test)\nthreshold = 0.5\ntest_pred = np.where(test_pred_prob > threshold, 1,0)\ntest_pred = np.squeeze(test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip((test_df.id.values, test_pred))\n\ntest_ids = test_df.id.values\ntest_pred_df = pd.DataFrame({'id':test_ids, 'target':test_pred})\ntest_pred_df.to_csv(\"submission.csv\", index=False)\n#test_ids.shape, test_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test the impact of initial bias initialization","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nEPOCHS = 10\nBATCH_SIZE = 64\ndo_analyze_bias_init = False\nif do_analyze_bias_init:\n    zero_bias_model = make_model(SEQ_LEN, VOCAB_SIZE, 64, METRICS)\n    zero_bias_model.layers[-1].bias.assign([0.0])\n    print(\"zeros bias\")\n    zero_bias_history = zero_bias_model.fit(\n        X_train,\n        y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,    \n        validation_data=(X_val, y_val), \n        verbose=1)\n    print(\"careful bias\")\n    care_bias_model = make_model(SEQ_LEN, VOCAB_SIZE, 64, METRICS, initial_bias)\n    careful_bias_history = care_bias_model.fit(\n        X_train,\n        y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_data=(X_val, y_val),\n        verbose=1)\n    \n    plot_metric(zero_bias_history, \"Zero Bias\",'loss', 0)\n    plot_metric(careful_bias_history, \"Careful Bias\",'loss', 1)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.max(), ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}