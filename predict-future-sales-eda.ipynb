{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"predict-future-sales-eda.ipynb","provenance":[],"collapsed_sections":["Ugz10Ex91nwR","RBhEUCmo1nwT"]}},"cells":[{"cell_type":"code","metadata":{"id":"974fYi-17v21"},"source":["# get the keys from your kaggle account information\n","# my account > API > Create New API Token\n","# get the content of the file \n","import json\n","token = {\"username\":\"jeankunz\",\"key\":\"96471399d8756c0424015f6343ff63c3\"}\n","with open('/content/kaggle.json', 'w') as file:\n","    json.dump(token, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1Hj9RZj7WPb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634555145759,"user_tz":-120,"elapsed":5376,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"582fcec0-f1ef-475f-a2b9-efe26833dd27"},"source":["%%bash\n","\n","if [ ! -f /content/sales_train.csv.zip ]; then    \n","    #mkdir ~/.kaggle\n","    #cp -i /content/.kaggle/kaggle.json  ~/.kaggle/\n","    cp kaggle.json ~/.kaggle/\n","    chmod 600 /content/kaggle.json\n","    #cat /root/.kaggle/kaggle.json\n","    #kaggle config set -n path -v{/content}\n","    \n","    kaggle competitions download -c competitive-data-science-predict-future-sales \n","    \n","    unzip -o /content/sales_train.csv.zip -d /content\n","    unzip -o /content/test.csv.zip -d /content\n","    unzip -o /content/items.csv.zip -d /content\n","    rm /content/sales_train.csv.zip\n","    rm /content/test.csv.zip\n","    rm /content/items.csv.zip\n","fi\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n","Downloading shops.csv to /content\n","\n","Downloading item_categories.csv to /content\n","\n","Downloading sales_train.csv.zip to /content\n","\n","Downloading test.csv.zip to /content\n","\n","Downloading sample_submission.csv.zip to /content\n","\n","Downloading items.csv.zip to /content\n","\n","Archive:  /content/sales_train.csv.zip\n","  inflating: /content/sales_train.csv  \n","Archive:  /content/test.csv.zip\n","  inflating: /content/test.csv       \n","Archive:  /content/items.csv.zip\n","  inflating: /content/items.csv      \n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0.00/2.91k [00:00<?, ?B/s]\r100%|##########| 2.91k/2.91k [00:00<00:00, 7.49MB/s]\n","\r  0%|          | 0.00/3.49k [00:00<?, ?B/s]\r100%|##########| 3.49k/3.49k [00:00<00:00, 3.99MB/s]\n","\r  0%|          | 0.00/13.3M [00:00<?, ?B/s]\r 68%|######7   | 9.00M/13.3M [00:00<00:00, 39.9MB/s]\r100%|##########| 13.3M/13.3M [00:00<00:00, 51.9MB/s]\n","\r  0%|          | 0.00/1.02M [00:00<?, ?B/s]\r100%|##########| 1.02M/1.02M [00:00<00:00, 145MB/s]\n","\r  0%|          | 0.00/468k [00:00<?, ?B/s]\r100%|##########| 468k/468k [00:00<00:00, 147MB/s]\n","\r  0%|          | 0.00/368k [00:00<?, ?B/s]\r100%|##########| 368k/368k [00:00<00:00, 160MB/s]\n"]}]},{"cell_type":"code","metadata":{"_uuid":"32b95bce-ab92-4abc-9e7d-8dff95872b30","_cell_guid":"56d9fd69-fcec-4682-a871-a4e161b90aee","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-19T09:35:25.399439Z","iopub.execute_input":"2021-09-19T09:35:25.399811Z","iopub.status.idle":"2021-09-19T09:36:47.271883Z","shell.execute_reply.started":"2021-09-19T09:35:25.399778Z","shell.execute_reply":"2021-09-19T09:36:47.270456Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7Knet-1A1nv-","executionInfo":{"status":"ok","timestamp":1634555257172,"user_tz":-120,"elapsed":48156,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"e446d8d3-c3e0-4868-814c-d30cff3b6179"},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import tensorflow as tf\n","import functools\n","\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","print(tf.__version__)\n","\n","sales_df = pd.read_csv(\"/content/sales_train.csv\")\n","items_df = pd.read_csv(\"/content/items.csv\")\n","item_cats_df = pd.read_csv(\"/content/item_categories.csv\")\n","shops_df = pd.read_csv(\"/content/shops.csv\")\n","test_df = pd.read_csv(\"/content/test.csv\")\n","\n","train_df = sales_df.merge(items_df[['item_id','item_category_id']], left_on='item_id', right_on='item_id', how='left')\n","\n","sales_month_df = train_df.sort_values(['shop_id','item_id','date_block_num']).groupby(['shop_id','item_id','date_block_num'])[['item_cnt_day']].sum().reset_index()\n","\n","test_shop_ids = set(test_df.shop_id.unique())\n","train_shop_ids = set(sales_month_df.shop_id.unique())\n","# check if some shops, items, are not in train\n","test_item_ids = set(test_df.item_id.unique())\n","train_item_ids = set(sales_month_df.item_id.unique())\n","all_item_ids = set(items_df.item_id.unique())\n","#del tmp_sales_months_df\n","assert len(train_item_ids) < len(all_item_ids)\n","\n","month_nbs = np.arange(0,sales_month_df.date_block_num.max()+1)\n","shops = shops_df.shop_id.unique()\n","# we first work only with item in train. Then we will add the one that are in test only (with values inherited from their categories)\n","items = sales_month_df.item_id.unique()\n","\n","\n","shop_item_ids = sales_month_df.drop_duplicates(subset = ['shop_id','item_id'])[['shop_id','item_id']].values.tolist()\n","\n","\n","index_tuples = [(shop_item[0], shop_item[1], m) for shop_item in shop_item_ids for m in month_nbs ]\n","\n","\n","sales_idx = pd.MultiIndex.from_tuples(index_tuples, names=('shop_id','item_id','date_block_num'))\n","\n","#sales_idx = pd.MultiIndex.from_product([shops, items, month_nbs], names=('shop_id','item_id','date_block_num'))\n","\n","tmp_sales_months_df = sales_month_df.set_index(['shop_id',\n","                                                'item_id',\n","                                                'date_block_num']).reindex(sales_idx, \n","                                                                           fill_value=0.).reset_index()\n","# clean data memory\n","del sales_idx\n","del sales_month_df\n","del sales_df\n","del train_df\n","\n","\n","full_sales_month_df = tmp_sales_months_df.merge(items_df[['item_id','item_category_id']],\n","                            left_on='item_id',\n","                            right_on='item_id', \n","                            how='left')\n","\n","\n","len(test_item_ids), len(train_item_ids), len(all_item_ids), len(full_sales_month_df)\n","\n","\n","full_sales_month_df.query('shop_id==59 & item_id==30')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.6.0\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","      <th>date_block_num</th>\n","      <th>item_cnt_day</th>\n","      <th>item_category_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>14170826</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170827</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>1</td>\n","      <td>13.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170828</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>2</td>\n","      <td>10.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170829</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>3</td>\n","      <td>4.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170830</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170831</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>5</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170832</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>6</td>\n","      <td>1.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170833</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>7</td>\n","      <td>1.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170834</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>8</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170835</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>9</td>\n","      <td>1.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170836</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>10</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170837</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>11</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170838</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>12</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170839</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>13</td>\n","      <td>1.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170840</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>14</td>\n","      <td>3.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170841</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>15</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170842</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>16</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170843</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>17</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170844</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>18</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170845</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>19</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170846</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170847</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>21</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170848</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>22</td>\n","      <td>1.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170849</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>23</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170850</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>24</td>\n","      <td>1.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170851</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>25</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170852</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>26</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170853</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>27</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170854</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>28</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170855</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>29</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170856</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>30</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170857</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>31</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170858</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>32</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>14170859</th>\n","      <td>59</td>\n","      <td>30</td>\n","      <td>33</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          shop_id  item_id  date_block_num  item_cnt_day  item_category_id\n","14170826       59       30               0           0.0                40\n","14170827       59       30               1          13.0                40\n","14170828       59       30               2          10.0                40\n","14170829       59       30               3           4.0                40\n","14170830       59       30               4           0.0                40\n","14170831       59       30               5           0.0                40\n","14170832       59       30               6           1.0                40\n","14170833       59       30               7           1.0                40\n","14170834       59       30               8           0.0                40\n","14170835       59       30               9           1.0                40\n","14170836       59       30              10           0.0                40\n","14170837       59       30              11           0.0                40\n","14170838       59       30              12           0.0                40\n","14170839       59       30              13           1.0                40\n","14170840       59       30              14           3.0                40\n","14170841       59       30              15           0.0                40\n","14170842       59       30              16           0.0                40\n","14170843       59       30              17           0.0                40\n","14170844       59       30              18           0.0                40\n","14170845       59       30              19           0.0                40\n","14170846       59       30              20           0.0                40\n","14170847       59       30              21           0.0                40\n","14170848       59       30              22           1.0                40\n","14170849       59       30              23           0.0                40\n","14170850       59       30              24           1.0                40\n","14170851       59       30              25           0.0                40\n","14170852       59       30              26           0.0                40\n","14170853       59       30              27           0.0                40\n","14170854       59       30              28           0.0                40\n","14170855       59       30              29           0.0                40\n","14170856       59       30              30           0.0                40\n","14170857       59       30              31           0.0                40\n","14170858       59       30              32           0.0                40\n","14170859       59       30              33           0.0                40"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"qyLU1OM_1nwE"},"source":["## compute std and average so we can sample from the distribution for eache item, shop, month\n","we assume a normal distribution"]},{"cell_type":"markdown","metadata":{"id":"xerG5_DY1nwH"},"source":["### analyze test data"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:36:47.274078Z","iopub.execute_input":"2021-09-19T09:36:47.274610Z","iopub.status.idle":"2021-09-19T09:37:08.119225Z","shell.execute_reply.started":"2021-09-19T09:36:47.274568Z","shell.execute_reply":"2021-09-19T09:37:08.118452Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"JQjYdCVm1nwH","executionInfo":{"status":"ok","timestamp":1634555335351,"user_tz":-120,"elapsed":17160,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"847276af-264e-49a8-e0fd-7ca1d3dc6ef8"},"source":["# only for shop, item tuple in test that are not in train\n","test_shop_item_ids = test_df.drop_duplicates(subset = ['shop_id','item_id'])[['shop_id','item_id']].values.tolist()\n","\n","test_shop_item_set = set([(shop_item[0],shop_item[1]) for shop_item in test_shop_item_ids]) \n","train_shop_item_set = set([(shop_item[0],shop_item[1]) for shop_item in shop_item_ids])\n","\n","test_only_shop_items = list(test_shop_item_set.difference(train_shop_item_set))\n","train_test_shop_items = test_shop_item_set.union(train_shop_item_set)\n","len(test_only_shop_items)+ len(train_test_shop_items), len(test_shop_item_set.intersection(train_shop_item_set)), len(test_df)\n","\n","print(\"nb of items with no categories:\", len(items_df[items_df.item_category_id.isna()]))\n","\n","shop_cat_month_df = full_sales_month_df.groupby(['shop_id',\n","                                                 'item_category_id',\n","                                                 'date_block_num']).item_cnt_day.agg([np.mean, \n","                                                                                      np.std]).reset_index()\n","\n","test_index_tuples = [(shop_item[0], shop_item[1], m) for shop_item in test_only_shop_items for m in month_nbs ]\n","test_only_sales_idx = pd.MultiIndex.from_tuples(test_index_tuples, names=('shop_id','item_id','date_block_num'))\n","# create a dataframe of sales based on items that are only in test.\n","\n","test_only_sales_df = pd.DataFrame(index=test_only_sales_idx)\n","test_only_sales_df.reset_index(inplace=True)\n","\n","# test only shop, item tuple should not be in train df\n","def by_len(it):\n","    return len(full_sales_month_df.query(f\"shop_id=={it[0]} & item_id=={it[1]}\"))>0\n","assert len(list(filter( by_len, test_only_shop_items[:100])))==0\n","\n","test_only_sales_cat_df = test_only_sales_df.merge(items_df[['item_id','item_category_id']], left_on='item_id', right_on='item_id', how='left')\n","\n","\n","test_only_sales_cat_df.head(10)\n","\n","test_only_sales_cat_avg_df = test_only_sales_cat_df.merge(shop_cat_month_df, \n","                                                          left_on=['shop_id','item_category_id','date_block_num'], \n","                                                          right_on=['shop_id','item_category_id','date_block_num'])\n","assert len(test_only_sales_cat_avg_df== len(test_only_sales_cat_df))\n","\n","test_only_sales_cat_avg_df.query('(shop_id==22) &(item_id==13463)').head(10)\n","\n","np.random.seed(42)\n","test_only_sales_cat_avg_df['item_cnt_day_sampled']=np.random.normal(test_only_sales_cat_avg_df['mean'].values, \n","                                                            test_only_sales_cat_avg_df['std'].values)\n","\n","test_only_sales_cat_avg_df.query('(shop_id==22) &(item_id==13463)').head(10)\n","\n","test_only_sales_cat_avg_df.loc[:, 'item_cnt_day']=test_only_sales_cat_avg_df.item_cnt_day_sampled.round(0).replace([-0],0)\n","test_only_item_id = test_only_sales_cat_avg_df.item_id.unique()[10]\n","assert len(full_sales_month_df.query(f'(shop_id==0)&(item_id=={test_only_item_id})'))==0, \"in full_sales_month_df, there should not be items in test only\"\n","\n","# columns should be the same\n","# Concatenate avg based sales for test only items and regular items\n","assert len(set(full_sales_month_df.columns).difference(set(test_only_sales_cat_avg_df.columns)))==0\n","all_sales_df = pd.concat([full_sales_month_df, test_only_sales_cat_avg_df[list(full_sales_month_df.columns.values)]])\n","assert len(all_sales_df)== len(full_sales_month_df)+len(test_only_sales_cat_avg_df)\n","assert len(all_sales_df.item_id.unique())==len(items_df.item_id.unique())\n","\n","all_sales_df.item_cnt_day.fillna(0., inplace=True)\n","\n","del test_only_sales_cat_avg_df\n","del test_only_sales_cat_df\n","del test_only_sales_df\n","del test_only_sales_idx\n","del full_sales_month_df"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nb of items with no categories: 0\n"]}]},{"cell_type":"markdown","metadata":{"id":"akaMhht41nwI"},"source":["## Train-Validation split\n","Train on before last 3 month.\n","Validate on last 3 months."]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:08.120363Z","iopub.execute_input":"2021-09-19T09:37:08.121388Z","iopub.status.idle":"2021-09-19T09:37:09.109754Z","shell.execute_reply.started":"2021-09-19T09:37:08.121345Z","shell.execute_reply":"2021-09-19T09:37:09.108872Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"BjFEj50c1nwJ","executionInfo":{"status":"ok","timestamp":1634555361114,"user_tz":-120,"elapsed":1182,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"0b4d9830-df68-45d6-f4c5-779daf3cdcb3"},"source":["train_df = all_sales_df[all_sales_df.date_block_num<33]\n","val_df = all_sales_df\n","\n","all_sales_df.date_block_num.max(), train_df.date_block_num.max()\n","print(train_df[(train_df.shop_id==3)&(train_df.item_id==138)])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["         shop_id  item_id  date_block_num  item_cnt_day  item_category_id\n","2350176        3      138               0           0.0                45\n","2350211        3      138               1           0.0                45\n","2350246        3      138               2           0.0                45\n","2350281        3      138               3           0.0                45\n","2350316        3      138               4           0.0                45\n","2350351        3      138               5           0.0                45\n","2350386        3      138               6           0.0                45\n","2350421        3      138               7           0.0                45\n","2350456        3      138               8           0.0                45\n","2350491        3      138               9           0.0                45\n","2350526        3      138              10           0.0                45\n","2350561        3      138              11           0.0                45\n","2350596        3      138              12           0.0                45\n","2350631        3      138              13           0.0                45\n","2350666        3      138              14           0.0                45\n","2350701        3      138              15           0.0                45\n","2350736        3      138              16           0.0                45\n","2350771        3      138              17           0.0                45\n","2350806        3      138              18           0.0                45\n","2350841        3      138              19           0.0                45\n","2350876        3      138              20           0.0                45\n","2350911        3      138              21           0.0                45\n","2350946        3      138              22           0.0                45\n","2350981        3      138              23           0.0                45\n","2351016        3      138              24           0.0                45\n","2351051        3      138              25           0.0                45\n","2351086        3      138              26           0.0                45\n","2351121        3      138              27           0.0                45\n","2351156        3      138              28           0.0                45\n","2351191        3      138              29           0.0                45\n","2351226        3      138              30           0.0                45\n","2351261        3      138              31           0.0                45\n","2351296        3      138              32           0.0                45\n"]}]},{"cell_type":"markdown","metadata":{"id":"O_-TOT0Y1nwJ"},"source":["## Prepare features"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:09.111912Z","iopub.execute_input":"2021-09-19T09:37:09.112740Z","iopub.status.idle":"2021-09-19T09:37:09.132097Z","shell.execute_reply.started":"2021-09-19T09:37:09.112698Z","shell.execute_reply":"2021-09-19T09:37:09.131026Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"g4Kfm3wN1nwK","executionInfo":{"status":"ok","timestamp":1634555363081,"user_tz":-120,"elapsed":281,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"baaa0fa8-8d2a-4ed5-fd59-8701cf6c5f3a"},"source":["WIN_SIZE=5\n","def prep_win_x(x, win_size=None,):\n","    if win_size:\n","        l = x.shape[0]-1\n","        to = l\n","        fr = to-win_size\n","        #print(l, fr, to)\n","        win_x = x[fr:to]\n","        #win_y = x[to:, y_col_idx]\n","    else:\n","        win_x = x[:-1]\n","        #win_y = x[-1:,y_col_idx]\n","    return win_x\n","\n","def prep_y(x, y_col_idx=None):\n","    return x[-1:,y_col_idx][0]\n","\n","tst_x = [np.array([[59., 30.,  1., 13., 0],\n"," [59., 30.,  2., 10., 1],\n"," [59., 30.,  3.,  4., 2],\n"," [59., 30.,  4.,  0., 3],\n"," [59., 30.,  5.,  0., 4],\n"," [59., 30.,  6.,  1., 5],\n"," [59., 30.,  7.,  1., 6]]),\n"," np.array([[59., 30.,  1., 13., 0],\n"," [59., 30.,  2., 10., 1],\n"," [59., 30.,  3.,  4., 2],\n"," [59., 30.,  4.,  0., 3],\n"," [59., 30.,  5.,  0., 4],\n"," [59., 30.,  6.,  1., 5],\n"," [59., 30.,  7.,  1., 6]])]\n","\n","res_win=list(map(functools.partial(prep_win_x, win_size=5), tst_x))\n","assert res_win[0].shape[0]==5\n","res = list(map(functools.partial(prep_win_x), tst_x))\n","assert res[0].shape[0]==6\n","\n","list(map(functools.partial(prep_y, y_col_idx=4), tst_x))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[6.0, 6.0]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:09.133490Z","iopub.execute_input":"2021-09-19T09:37:09.133947Z","iopub.status.idle":"2021-09-19T09:37:42.294462Z","shell.execute_reply.started":"2021-09-19T09:37:09.133899Z","shell.execute_reply":"2021-09-19T09:37:42.293604Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"sC4BFL131nwK","executionInfo":{"status":"ok","timestamp":1634555386439,"user_tz":-120,"elapsed":22175,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"8de8b226-a113-4aca-fcda-297e4ac2ea6f"},"source":["def create_win(df, win_size=30):\n","    arr = df[['shop_id','item_id','item_category_id','date_block_num', 'item_cnt_day']].sort_values(['shop_id','item_id','date_block_num']).values    \n","    groups_idx = np.unique(arr[:,:2], axis=0, return_index=True)\n","    #print(groups_idx)\n","    group_list = np.split(arr, groups_idx[1][1:])\n","    #print(group_list)\n","    \n","    x = np.array(list(map(functools.partial(prep_win_x, win_size=SEQLEN),group_list[:] )))\n","    y = np.array(list(map(functools.partial(prep_y, y_col_idx=4),group_list[:] )))\n","    \n","    return x, y\n","\n","SEQLEN=30\n","train_x, train_y = create_win(train_df[:], win_size=SEQLEN)\n","assert train_x.shape[0]== train_y.shape[0]    \n","\n","val_x, val_y = create_win(val_df, win_size=SEQLEN)\n","assert val_x.shape[0]== val_y.shape[0]    \n","train_x.shape, train_y.shape, val_x.shape, val_y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((502783, 30, 5), (502783,), (502783, 30, 5), (502783,))"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:42.295592Z","iopub.execute_input":"2021-09-19T09:37:42.295824Z","iopub.status.idle":"2021-09-19T09:37:42.616000Z","shell.execute_reply.started":"2021-09-19T09:37:42.295798Z","shell.execute_reply":"2021-09-19T09:37:42.615158Z"},"trusted":true,"id":"qR8JRxuo1nwL"},"source":["train_x[:1], train_y[:1]\n","assert np.argwhere(np.isnan(train_x)).shape[0]==0, \"there should be no nan value in train\"\n","#train_x[13729]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:42.617112Z","iopub.execute_input":"2021-09-19T09:37:42.617339Z","iopub.status.idle":"2021-09-19T09:37:43.395569Z","shell.execute_reply.started":"2021-09-19T09:37:42.617314Z","shell.execute_reply":"2021-09-19T09:37:43.394726Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"O3_wTScn1nwL","executionInfo":{"status":"ok","timestamp":1634555387426,"user_tz":-120,"elapsed":576,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"13f36160-6064-41ba-ad89-5bd8aeca6d13"},"source":["def prep_seq_x(x):\n","    \n","    cols =['shop_id','item_id', 'item_category_id', 'month_nb','sales']\n","    \n","    cat_cols = ['shop_id','item_id', 'item_category_id']\n","    seq_cat_cols = ['month_nb']\n","    seq_cols = ['sales']\n","    pred_col = 'sales'\n","    X_cat = x[:, :, np.where(np.isin(cols, cat_cols))].squeeze()\n","    X_seq = x[:, :, np.where(np.isin(cols, seq_cols))].squeeze(-1)\n","    # create embeddings for each month\n","    X_seq_cat = x[:, :, np.where(np.isin(cols, seq_cat_cols))].squeeze(-1)\n","\n","    return X_cat, X_seq, X_seq_cat\n","\n","\n","train_X_cat, train_X_seq, train_X_seq_cat = prep_seq_x(train_x[:],)\n","val_X_cat, val_X_seq, val_X_seq_cat = prep_seq_x(val_x[:],)\n","train_X_cat.shape, train_X_seq.shape, train_X_seq_cat.shape, train_y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((502783, 30, 3), (502783, 30, 1), (502783, 30, 1), (502783,))"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:43.396662Z","iopub.execute_input":"2021-09-19T09:37:43.396865Z","iopub.status.idle":"2021-09-19T09:37:43.402705Z","shell.execute_reply.started":"2021-09-19T09:37:43.396842Z","shell.execute_reply":"2021-09-19T09:37:43.401892Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"JdQp9Zvl1nwM","executionInfo":{"status":"ok","timestamp":1634555507107,"user_tz":-120,"elapsed":288,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"d070d018-8d65-41a0-8cdb-9969714af550"},"source":["train_X_seq.squeeze().shape, train_X_seq.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((502783, 30), (502783, 30, 1))"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:43.403722Z","iopub.execute_input":"2021-09-19T09:37:43.403943Z","iopub.status.idle":"2021-09-19T09:37:44.983963Z","shell.execute_reply.started":"2021-09-19T09:37:43.403919Z","shell.execute_reply":"2021-09-19T09:37:44.983115Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"v974JWty1nwM","executionInfo":{"status":"ok","timestamp":1634555510019,"user_tz":-120,"elapsed":1983,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"c38f9c3f-dc98-491f-9c22-7fe0fb61f121"},"source":["# baseline with linear model\n","from sklearn.linear_model import LinearRegression\n","\n","lr = LinearRegression()\n","train_X = train_X_seq.squeeze()\n","val_X = val_X_seq.squeeze()\n","lr.fit(train_X, train_y)\n","lr.score(train_X, train_y), lr.score(val_X, val_y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.18285292263103536, -2.423061694620623)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:45.796508Z","iopub.execute_input":"2021-09-19T09:37:45.796868Z","iopub.status.idle":"2021-09-19T09:37:45.806411Z","shell.execute_reply.started":"2021-09-19T09:37:45.796820Z","shell.execute_reply":"2021-09-19T09:37:45.805232Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"K2M7pF-o1nwN","executionInfo":{"status":"ok","timestamp":1634555510020,"user_tz":-120,"elapsed":10,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"d514ca06-f4ca-40a3-c77b-a4598a9686c4"},"source":["train_X_cat.shape, train_X_seq_cat.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((502783, 30, 3), (502783, 30, 1))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:39:31.400539Z","iopub.execute_input":"2021-09-19T09:39:31.400791Z","iopub.status.idle":"2021-09-19T09:39:37.025175Z","shell.execute_reply.started":"2021-09-19T09:39:31.400762Z","shell.execute_reply":"2021-09-19T09:39:37.024286Z"},"trusted":true,"id":"LBYwmTtP1nwP"},"source":["# detect and init the TPU\n","#tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n","\n","# instantiate a distribution strategy\n","#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:40:26.730970Z","iopub.execute_input":"2021-09-19T09:40:26.731419Z","iopub.status.idle":"2021-09-19T09:40:27.121008Z","shell.execute_reply.started":"2021-09-19T09:40:26.731388Z","shell.execute_reply":"2021-09-19T09:40:27.119949Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"W3-q00FA1nwQ","executionInfo":{"status":"error","timestamp":1634554911509,"user_tz":-120,"elapsed":838,"user":{"displayName":"jean kunz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6TaBzIQ4mQWutdoLL2u5S87mX6R2liFFsfj7Be2c=s64","userId":"06006990243808902630"}},"outputId":"688cb5dd-ac06-44c9-cc46-dd9f5e60a481"},"source":["class SequenceModel(tf.keras.Model):\n","    ...\n","    def __init__(self, seq_len, shop_nb, item_nb, month_nb=SEQLEN, size=64, dropout=0.5, output_bias=None):\n","        super(SequenceModel, self).__init__()\n","        self.size = size\n","        self.shop_nb = shop_nb\n","        self.item_nb = item_nb\n","        self.month_nb = month_nb\n","        \n","            # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#checkpoint_the_initial_weights\n","        if output_bias is not None:\n","            output_bias = tf.keras.initializers.Constant(output_bias)\n","        \n","        self.i = tf.keras.layers.Input(shape=(SEQLEN,1))\n","        self.rnn = tf.keras.layers.GRU(64, activation='relu')\n","        self.dnn = tf.keras.layers.Dense(1)        \n","    \n","    def call(self, X):                \n","        i = self.i(X)\n","        out = self.rnn(i)\n","        y = self.dnn(out)\n","        \n","        return y\n","\n","\n","        \n","def get_model(seq_len, shop_nb, item_nb, month_nb=SEQLEN, size=64, dropout=0.5, output_bias=None):\n","    \n","    seq_input = tf.keras.layers.Input(shape=(month_nb,1))\n","    x = tf.keras.layers.Flatten()(seq_input)\n","    #x = tf.keras.layers.GRU(64, activation='relu')(seq_input)\n","    out = tf.keras.layers.Dense(1)(x)\n","    model = tf.keras.Model(inputs=seq_input, outputs=out)  \n","\n","    model = tf.keras.Sequential(\n","        [            \n","            tf.keras.layers.GRU(64, activation=\"relu\"),\n","            tf.keras.layers.Dense(1)        \n","        ]\n","    )\n","    \n","    \n","    model.compile(loss=tf.keras.losses.Huber(),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","    \n","    return model\n","\n","\n","#tf.debugging.set_log_device_placement(False)\n","shop_nb = len(shops)\n","item_nb = len(items)\n","\n","model = get_model(SEQLEN, shop_nb, item_nb)\n","BATCH_SIZE=128\n","    \n","train_ds = tf.data.Dataset.from_tensor_slices((train_X_seq[:10000], train_y[:10000]))\n","\n","model.build(input_shape=(None,SEQLEN,1))\n","model.summary()\n","model.fit(train_ds.batch(BATCH_SIZE), epochs=5, verbose=1)\n","#model.fit(train_X_seq[:1000], train_y[:1000], batch_size=64, epochs=1)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-4114de55f13a>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    tf.keras.y = tf.keras.layers.Conv1D(20, 3, activation='relu',input_shape=input_shape[1:])(x)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:29:01.708773Z","iopub.status.idle":"2021-09-13T12:29:01.709639Z"},"trusted":true,"id":"gvtg9ng-1nwQ"},"source":["train_y[:100].shape, train_X_seq[:100].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:29:01.710869Z","iopub.status.idle":"2021-09-13T12:29:01.711448Z"},"trusted":true,"id":"4AiKAhv41nwQ"},"source":["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDA1RqSr8Mz_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eIubL9AP8NQz"},"source":["## transform to tf-records (not required)\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:37:45.808436Z","iopub.execute_input":"2021-09-19T09:37:45.808852Z","iopub.status.idle":"2021-09-19T09:39:30.886726Z","shell.execute_reply.started":"2021-09-19T09:37:45.808808Z","shell.execute_reply":"2021-09-19T09:39:30.885529Z"},"trusted":true,"id":"zKY3LKqU1nwO"},"source":["def to_tf_records(X_cat, X_seq, X_seq_cat,y, file_name):\n","    with tf.io.TFRecordWriter(file_name) as writer: \n","        for i, item in enumerate(X_seq.tolist()):\n","            x_cat_ser = tf.io.serialize_tensor(X_cat[i])  \n","            #print(x_cat_ser.numpy())\n","            ex_schema = {                \n","                'x_cat': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(X_cat[i]).numpy()])),\n","                'x_seq': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(X_seq[i]).numpy()])),\n","                # another way to encode 1d lists\n","                #'x_seq': tf.train.Feature(float_list=tf.train.FloatList(value=X_seq[i])),\n","                'x_seq_cat': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(X_seq_cat[i]).numpy()])),\n","                #'x_seq_cat': tf.train.Feature(int64_list=tf.train.Int64List(value=X_seq_cat[i])),\n","                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y[i]]))\n","            }\n","            ex = tf.train.Example(features=tf.train.Features(feature=ex_schema))      \n","            writer.write(ex.SerializeToString())\n","                                  \n","                                  \n","nb_ex = 1000000  #train_X_cat.shape[0]\n","to_tf_records(train_X_cat[:nb_ex],train_X_seq[:nb_ex], train_X_seq_cat[:nb_ex],train_y[:nb_ex],'/content/train_seq.proto')                                  \n","def parse_tfrecord_fn(example):\n","    feat_desc = {\n","        'x_cat': tf.io.FixedLenFeature([], dtype=tf.string),\n","        'x_seq': tf.io.FixedLenFeature([], dtype=tf.string),\n","        #'x_seq': tf.io.VarLenFeature(tf.float32),\n","        'x_seq_cat': tf.io.FixedLenFeature([], dtype=tf.string),        \n","        'y': tf.io.FixedLenFeature([], dtype=tf.float32)\n","    }    \n","    example = tf.io.parse_single_example(example, feat_desc)\n","    # other way to decode 1d list, if encoded that way\n","    #example[\"x_seq\"] = tf.sparse.to_dense(example[\"x_seq\"])    \n","    example['x_seq'] = tf.ensure_shape(tf.io.parse_tensor(example['x_seq'], out_type=tf.float64),(SEQLEN,1))    \n","    example['x_seq_cat']= tf.ensure_shape(tf.io.parse_tensor(example['x_seq_cat'], out_type=tf.float64), (SEQLEN, 1))\n","    example['x_cat'] = tf.ensure_shape(tf.io.parse_tensor(example['x_cat'], out_type=tf.float64), (SEQLEN, 3))\n","    example['y'] = tf.ensure_shape(example['y'], ())\n","    return example\n","\n","\n","debug_ex = False\n","raw_dataset = tf.data.TFRecordDataset(['/content/train_seq.proto'])\n","for features in raw_dataset.map(parse_tfrecord_fn).batch(batch_size=2):\n","    for key in features.keys():       \n","        if debug_ex:\n","            print(f\"{key}: {features[key]}\")\n","    break\n","\n","\n","def prepare_sample(features):    \n","    ''' filter and format data to given model'''     \n","    #x_seq = tf.ensure_shape(features['x_seq'], (1, SEQLEN,1))   \n","    x_seq = features['x_seq']\n","    y = tf.ensure_shape(features['y'],())\n","   #print(\"x seq\", x_seq.shape, y.shape)\n","    return x_seq, y\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","def get_dataset(filenames, batch_size):\n","    \n","    ## to optimize pipelinening instead of tf.data.TFRecord...\n","    #(tf.data.Dataset.list_files(filenames)\n","    #.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE))\n","    \n","    dataset = (\n","        #tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n","        tf.data.Dataset.list_files(filenames)\n","        .interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","        .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n","        .map(prepare_sample, num_parallel_calls=AUTOTUNE)\n","        .shuffle(batch_size * 10)\n","        .batch(batch_size)\n","        .prefetch(AUTOTUNE)\n","    )\n","    return dataset\n","\n","#get_dataset('data/*.proto', batch_size=2)\n","ds = get_dataset('/content/*.proto', batch_size=4)\n","X_batch, y_batch = next(iter(ds))\n","for i, row in enumerate(iter(ds)):\n","    print(i, row[0].shape, row[1].shape)\n","    if i>10:        \n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:29:01.712341Z","iopub.status.idle":"2021-09-13T12:29:01.712912Z"},"trusted":true,"id":"DsH1O7Sw1nwR"},"source":["\n","#model.build((None, SEQLEN, 1))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"xGg7T_PT1nwR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ugz10Ex91nwR"},"source":["## Model\n","\n","- don't forget to add a out of vocab categories\n","- no need to rescale/std values, because we only have category and the value to regress (sales prediction)"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:29:01.71373Z","iopub.status.idle":"2021-09-13T12:29:01.714282Z"},"trusted":true,"id":"vZvUQufD1nwR"},"source":["# compute regression value mean to init bias\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"s37YplWs1nwS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"KWjAxMhP1nwS"},"source":["model.evaluate(val_X_seq, val_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXrSgxUF1nwS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"11339b2f-f4eb-491b-90cf-31bfa45732a6","_cell_guid":"39aa147c-e2dc-4096-b70e-436280143fa0","jupyter":{"outputs_hidden":false},"trusted":true,"id":"84qu3Lv41nwS"},"source":["win_X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"SVMUxXxA1nwS"},"source":["#https://www.youtube.com/watch?v=ZnukSLKEw34\n","ds = tf.data.Dataset.list_files('data/*.proto')\n","ds = ds.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","#ds = tf.data.TFRecordDataset(['data/*.proto'])\n","ds = ds.map(decode_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","ds = ds.batch(batch_size=2)\n","# to enable pipelining\n","ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"v_GFrXho1nwT"},"source":["t1 = [[1, 2]]\n","t2 = [[7, 8]]\n","nonscalar = tf.concat([t1, t2], 0)\n","nonscalar_np = np.array([t1,t2])\n","print(nonscalar)\n","print(\"nonscalarnonscalar\", nonscalar)\n","\n","serialized_nonscalar = tf.io.serialize_tensor(nonscalar)\n","\n","print(\"serialized_nonscalar\", serialized_nonscalar.numpy())\n","# to numpy to make it bytes instead of tensor\n","feature_of_bytes = tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_nonscalar.numpy()]))\n","\n","\n","print(\"feat of bytes\",feature_of_bytes)\n","features_for_example = {\n","  'x1': feature_of_bytes\n","}\n","example_proto = tf.train.Example(features=tf.train.Features(feature=features_for_example))\n","print(\"example\",example_proto)\n","\n","# will be written to tf.record file.\n","ex_string = example_proto.SerializeToString()\n","print(\"serialized ex in string\", ex_string)\n","ex_from_str = tf.train.Example.FromString(ex_string)\n","print(\"ex from string\", ex_from_str)\n","\n","feat_desc = {\"x1\": tf.io.FixedLenFeature([], dtype=tf.string)}\n","\n","# it takes a serialized string as input, like a row in a tf.record.\n","parsed_ex = tf.io.parse_single_example(ex_string, feat_desc)\n","print(\"parsed ex\", parsed_ex)\n","print(\"parsed x1 feat as tensor\", parsed_ex['x1'])\n","\n","x1_str = parsed_ex['x1'].numpy()\n","print(\"x1 as str\", x1_str)\n","\n","(tf.io.parse_tensor(x1_str, out_type=tf.int32) == nonscalar)\n","tf.io.parse_tensor(x1_str, out_type=tf.int32)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"543a5499-d62f-451c-afd8-ad2b03c6d44e","_cell_guid":"9b2c483c-e4bd-4d29-81ea-355604a1d545","trusted":true,"id":"RBhEUCmo1nwT"},"source":["## Windowing"]},{"cell_type":"code","metadata":{"trusted":true,"id":"pZQ1yVBG1nwT"},"source":["# no need to pad because we got a 33 fixed size len \n","def rolling_win(x, y=None, win_len=1, verbose=False):    \n","    '''\n","    y can be none, when we create window in test mode.\n","    '''\n","    nb_row = x.shape[0]\n","    nb_col = x.shape[1]\n","    if verbose:\n","        print('nb row', nb_row)\n","        print('nb col', nb_col)\n","        \n","    win_nb = nb_row - win_len + 1\n","        \n","    win_len_dim = np.expand_dims(\n","        np.arange(0, win_len), 0)\n","    win_nb_dim = np.expand_dims(np.arange(win_nb), 0).T\n","    win_idx = win_len_dim + win_nb_dim\n","    win_x = x[win_idx]\n","    if verbose:\n","        print(\"win nb\", win_nb)\n","        print(\"win len dim\", win_len_dim)\n","        print(\"win nb dim\", win_nb_dim)\n","        print(\"win idx\", win_idx)\n","        print(\"win x\", win_x)\n","        \n","    win_y = None\n","    if y is not None:\n","        y_idx = np.arange(win_len, win_len+win_nb-1)\n","        win_y = y[y_idx]\n","        if verbose:\n","            print(\"y idx\", y_idx)\n","            print(\"win y\", win_y)\n","    \n","        \n","    return win_x[:-1], win_y\n","    \n","    \n","\n","tst_x = np.array([[59., 30.,  1., 13.],\n"," [59., 30.,  2., 10.],\n"," [59., 30.,  3.,  4.],\n"," [59., 30.,  4.,  0.],\n"," [59., 30.,  5.,  0.],\n"," [59., 30.,  6.,  1.],\n"," [59., 30.,  7.,  1.]])\n","\n","tst_y = np.expand_dims(np.array([0,1,2,3,4,5,6]),axis=1)\n","#tst_y = np.array([[0,1,2,3,4,5,6]])\n","\n","tst_win_x, tst_win_y = rolling_win(tst_x, tst_y, win_len=3, verbose=False)\n","#tst_win_x.shape, tst_win_y.shape, tst_y.shape\n","\n","\n","\n","# new approach -> one time serie window per group as input of model\n","\n","def prep_win(x):\n","    win_x, win_y = rolling_win(x[:-1], x[1:,4], win_len= 29 )\n","    return (win_x, win_y)\n","\n","it = map(prep_win, train_groups[:100])\n","tmp = list(it)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"bd2d00f4-5377-4f67-9868-cb6445b596f8","_cell_guid":"4223c459-58d7-49d5-a9ac-e792857e2746","jupyter":{"outputs_hidden":false},"trusted":true,"id":"yitEs_ew1nwT"},"source":["#%%timeit\n","def prep_win(df, win_len=10):\n","        grp_df = df.sort_values(['shop_id', 'item_id','date_block_num']).groupby(['shop_id','item_id'])\n","        all_x = None\n","        all_y = None\n","        for name, grp in grp_df:\n","            x = np.array(grp.values)\n","            y = np.expand_dims(np.array(grp.item_cnt_day.values),axis=1)\n","            #print(x.shape, y.shape)\n","            # we omit last row as we want to predict it (the goal is to predict next month sales).\n","            win_x, win_y = rolling_win(x,y, win_len=win_len, verbose=False)                        \n","            if all_x is None:\n","                all_x = win_x                \n","                all_y = win_y\n","                #print(all_x.shape, all_y.shape)\n","            else:\n","                all_x = np.concatenate((all_x, win_x), axis=0)\n","                all_y = np.concatenate((all_y, win_y), axis=0)\n","                #print(\">\",all_x.shape, all_y.shape, win_y.shape)\n","\n","        return all_x, all_y\n","            \n","\n","small_data = False\n","SEQ_LEN = 30\n","if small_data:\n","    sm_shops = [2,3, 5] #shops[:10]\n","    sm_items = [31,12]#items[:10]\n","\n","    df = train_df.loc[(train_df.shop_id.isin(sm_shops)) & (train_df.item_id.isin(sm_items))]\n","    seq, y = prep_win(df,win_len=SEQ_LEN)\n","else:\n","    df = train_df\n","    seq, y = prep_win(df,win_len=SEQ_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ExrbWWYK1nwU"},"source":["def prep_x_y(seq, seq_len):\n","    \n","    cols =['shop_id','item_id', 'month_nb','sales','item_category_id']\n","    cat_cols = ['shop_id','item_id', 'item_category_id']\n","    seq_cat_cols = ['month_nb']\n","    seq_cols = ['sales']\n","    pred_col = 'sales'\n","    X_cat = seq[:, :, np.where(np.isin(cols, cat_cols))].squeeze()\n","    X_seq = seq[:, :, np.where(np.isin(cols, seq_cols))].squeeze(-1)\n","    # create embeddings for each month\n","    X_seq_cat = seq[:, :, np.where(np.isin(cols, seq_cat_cols))].squeeze(-1)\n","    y = seq[:, :, cols.index(pred_col)].squeeze()[:, seq_len - 1]\n","\n","    return X_cat, X_seq, X_seq_cat,  y\n","\n","\n","X_cat, X_seq, X_seq_cat, y = prep_x_y(seq, SEQ_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"2536498e-6904-4610-8d9c-cf8e9bbadea6","_cell_guid":"b9ac1592-9597-4dd3-9595-03b3f86a9102","jupyter":{"outputs_hidden":false},"trusted":true,"id":"qkYDpVZs1nwU"},"source":["def rolling_win_padded(x, win_len, pad_nb=None, win_nb=None, verbose=False):\n","    '''\n","    Args:\n","        x: timeserie numpy array of shape (timestep, nb_features )\n","        win_len: len of window on timeserie\n","        pad_nb: the nb of left padded zeros (or first padded) that will be used to define first windows. If no pad_nb is provided, it will start to pad every timestep except the last one (which is the first item of window)\n","        win_nb: forced nb of window to be returned. If None, win_nb is computed.\n","        verbose: true if debug info is displayed\n","    '''\n","    nb_row = x.shape[0]\n","    nb_col = x.shape[1]\n","\n","    # if not defined then compute it.\n","    if win_nb is None:\n","        if nb_row <= win_len:\n","            pad_nb = win_len-nb_row\n","        elif pad_nb is None:\n","            pad_nb = 0\n","        # else we use pad_nb given as parameter.\n","        # nb of rows including pad\n","        nb_row_total = nb_row + pad_nb\n","        win_nb = nb_row_total - win_len + 1\n","        start_rolling_idx = 0\n","        if verbose:\n","            print(\"nb of windows is computed\")\n","\n","    else:\n","        # the len on which win will be rolled over.\n","        rolling_len = win_len + (win_nb-1)\n","        if nb_row >= rolling_len:\n","            pad_nb = 0\n","        else:\n","            pad_nb = rolling_len - nb_row\n","\n","        nb_row_total = nb_row + pad_nb\n","        start_rolling_idx = nb_row_total - rolling_len\n","        if verbose:\n","            print(\"nb of windows is forced\")\n","\n","    if verbose:\n","        print('> nb of windows', win_nb)\n","        print(\"> win len:\", win_len)\n","        print(\"> nb rows:\", nb_row)\n","        print(\"> nb rows total (with pad):\", nb_row_total)\n","        print(\"> nb col:\", nb_col)\n","        print(\"> start index (where we start rolling)\", start_rolling_idx)\n","\n","    pad = np.zeros((pad_nb, nb_col))\n","    padded_x = np.concatenate([pad, x])\n","\n","    if verbose:\n","        print(\"Nb row included pad:\", nb_row_total)\n","        print(\"nb windows:\", win_nb)\n","        print(\"\\n pad\", pad, \"\\n padded x\", padded_x, '\\n padd nb:', pad_nb)\n","    # -----\n","    # create a vectorized index based on a rolling index\n","    # rolling index defines windows with index pointing to data in x\n","    # we create a matrix of shape (win_len_dim, win_nb_dim)\n","    win_len_dim = np.expand_dims(\n","        np.arange(start_rolling_idx, win_len+start_rolling_idx), 0)\n","    win_nb_dim = np.expand_dims(np.arange(win_nb), 0).T\n","    if verbose:\n","        print(\"win dim\", win_len_dim, \"\\n nb of window dimension\", win_nb_dim)\n","    # we add timestep shift to first window of index\n","    win_idx = win_len_dim + win_nb_dim\n","    if verbose:\n","        print(\"rolling idx\", win_idx)\n","        print(\"rolling x\", padded_x[win_idx])\n","    return padded_x[win_idx]\n","\n","\n","x = np.array([[1, 'a', 1.5],\n","              [2, 'b', 3.2],\n","              [3, 'c', 3.5],\n","              [4, 'd', 3.3],\n","              [5, 'e', 5.2],\n","              [6, 'f', 8.2]])\n","\n","\n","rolling_x = rolling_win_padded(x[:, :], win_len=3, pad_nb=0, verbose=True)\n","assert rolling_x.shape == (4, 3, 3)\n","\n","rolling_x = rolling_win_padded(x[:, :], win_len=1, pad_nb=0)\n","assert rolling_x.shape == (6, 1, 3)\n","\n","rolling_x = rolling_win_padded(x[:, :], win_len=6, pad_nb=0)\n","assert rolling_x.shape == (1, 6, 3)\n","\n","rolling_x = rolling_win_padded(x[:, :], win_len=8, pad_nb=0)\n","assert rolling_x.shape == (1, 8, 3)\n","\n","\n","rolling_x = rolling_win_padded(x[:, :], win_len=8, pad_nb=2)\n","assert rolling_x.shape == (1, 8, 3)\n","\n","rolling_x = rolling_win_padded(x[:, :], win_len=8, win_nb=3, verbose=False)\n","assert rolling_x.shape == (3, 8, 3)\n","\n","rolling_x = rolling_win_padded(x[:, :], win_len=4, win_nb=2, verbose=False)\n","assert rolling_x.shape == (2, 4, 3)\n","assert rolling_x[-1, -1, 1] == 'f'  # last item is 6,'f',..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"38e40673-81bd-4972-bbb1-72af749092e0","_cell_guid":"f37e647b-a5b4-4ec3-848d-5daf2c89bdd6","trusted":true,"id":"BGWImg2z1nwU"},"source":["# %%time\n","from numpy import save, load\n","\n","INDEX_COL_USE_TO_PREDICT = 3\n","\n","\n","def prep_windowed_data(df, seq_len):\n","    grp_df = df.sort_values(['object_id', 'evt_at']).groupby('object_id')\n","    all_x = None\n","    min_nb_rows = 10\n","    for name, group in grp_df:\n","        #print(\"name\", name,\"values\", group.values)\n","        x = np.array(group.values)\n","        # we only select x for which the last timestep will have a use_to_predict==True, it's the first_true_idx\n","        first_true_idx = np.where(x[:, INDEX_COL_USE_TO_PREDICT] == True)[0][0]\n","        # compute the nb of window as nb of row - first_true_idx\n","        win_nb = x.shape[0] - first_true_idx\n","        win_x = rolling_win_padded(\n","            x[:], win_len=seq_len, win_nb=win_nb, verbose=False)\n","        if all_x is None:\n","            all_x = win_x\n","        else:\n","            all_x = np.concatenate((all_x, win_x), axis=0)\n","\n","    return all_x\n","\n","\n","# len of sequence (window)\n","SEQ_LEN = 10\n","MIN_NB_ROW_WIN = MIN_NB_EVT_PER_CPNY\n","\n","load_data = False\n","if load_data:\n","    train_seq = np_persistence_client.load_np_array(\n","        BASE_OUTPUTS_DIR+\"train_seq.npy\")\n","    test_seq = np_persistence_client.load_np_array(\n","        BASE_OUTPUTS_DIR+\"test_seq.npy\")\n","else:\n","    is_dev = False\n","    if is_dev:\n","        object_id_list = train_prep_df.object_id.unique()\n","        # object_id_list=['c:13529']\n","        # object_id_list=['c:1']\n","        #train_seq = prep_windowed_data(train_prep_df[train_prep_df.object_id.isin(object_id_list[:1000])], seq_len=SEQ_LEN)\n","        object_id_list = ['c:40456']\n","        object_id_list = ['c:104015']\n","        #object_id_list = test_prep_df.object_id.unique()\n","        test_seq = prep_windowed_data(test_prep_df[test_prep_df.object_id.isin(\n","            object_id_list[:1000])], seq_len=SEQ_LEN)\n","\n","    else:\n","        train_seq = prep_windowed_data(train_prep_df, seq_len=SEQ_LEN)\n","        np_persistence_client.save_np_array(\n","            train_seq, BASE_OUTPUTS_DIR+\"train_seq.npy\")\n","        # ---\n","        test_seq = prep_windowed_data(test_prep_df, seq_len=SEQ_LEN)\n","        np_persistence_client.save_np_array(\n","            test_seq, BASE_OUTPUTS_DIR+\"test_seq.npy\")\n","\n","        assert np.unique(test_seq[:, -1, INDEX_COL_USE_TO_PREDICT]) == [\n","            True], \"The last timestep must always have a use_to_predict==True in test\"\n","        assert np.unique(train_seq[:, -1, INDEX_COL_USE_TO_PREDICT]) == [\n","            True], \"The last timestep must always have a use_to_predict==True in train\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"380eba43-6e97-4522-a7aa-4ed2b78a94b2","_cell_guid":"ea78581e-412b-4459-8377-1a7717b7031f","jupyter":{"outputs_hidden":false},"trusted":true,"id":"Ge3f7Um_1nwV"},"source":["def prep_x_y(seq, cols, seq_len):\n","    '''\n","    return X for sequence, X for company static data and y\n","    '''\n","    seq_cols = [\n","        'cum_acquisition_nb', 'cum_rounds_nb', 'cum_raised_usd',\n","        'cum_nb_unique_investors', 'cum_nb_successfull_invest_by_investor',\n","        'nb_days_since_last_evt_type', 'nb_days_since_last_any_evt',\n","        'nb_days_since_first_evt', 'participants'\n","    ]\n","    #seq_cols = ['participants', 'cum_raised_usd', 'cum_nb_unique_investors', 'cum_nb_successfull_invest_by_investor', 'nb_days_since_last_any_evt']\n","    # try a model without cum counters\n","    seq_cat_cols = ['evt_type']\n","    cpny_cols = ['country_code', 'category_code']\n","    pred_col = 'success_horizon'\n","    pred_col = 'success_less_5_year'\n","    pred_col = 'success_less_2_year'\n","    assert seq.shape[-1] == len(\n","        cols), \"There should be as many columns (cols) as the last dimension of seq\"\n","    X_cpny = seq[:, :, np.where(np.isin(cols, cpny_cols))].squeeze()\n","    X_seq = seq[:, :, np.where(np.isin(cols, seq_cols))].squeeze()\n","    X_seq_cat = seq[:, :, np.where(np.isin(cols, seq_cat_cols))].squeeze(-1)\n","    y = seq[:, :, cols.index(pred_col)].squeeze()[:, seq_len - 1]\n","\n","    return X_cpny, X_seq, X_seq_cat, y\n","\n","\n","X_cpny_train, X_seq_train, X_seq_cat_train, y_train = prep_x_y(\n","    train_seq, cols, SEQ_LEN)\n","X_cpny_test, X_seq_test, X_seq_cat_test, y_test = prep_x_y(\n","    test_seq, cols, SEQ_LEN)"],"execution_count":null,"outputs":[]}]}