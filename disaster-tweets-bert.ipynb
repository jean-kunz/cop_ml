{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip list | grep tensorflow","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:04.260674Z","iopub.execute_input":"2021-05-31T11:48:04.261056Z","iopub.status.idle":"2021-05-31T11:48:06.862191Z","shell.execute_reply.started":"2021-05-31T11:48:04.260971Z","shell.execute_reply":"2021-05-31T11:48:06.861063Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"tensorflow                     2.4.1\ntensorflow-addons              0.12.1\ntensorflow-cloud               0.1.13\ntensorflow-datasets            3.0.0\ntensorflow-estimator           2.4.0\ntensorflow-gcs-config          2.1.7\ntensorflow-hub                 0.12.0\ntensorflow-metadata            0.29.0\ntensorflow-probability         0.12.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade --upgrade-strategy only-if-needed tensorflow-text==2.4.*\n!pip install -q tf-models-official==2.4.*","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:06.863769Z","iopub.execute_input":"2021-05-31T11:48:06.864115Z","iopub.status.idle":"2021-05-31T11:48:35.389705Z","shell.execute_reply.started":"2021-05-31T11:48:06.864073Z","shell.execute_reply":"2021-05-31T11:48:35.388693Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tensorflow-text==2.4.*\n  Downloading tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4 MB)\n\u001b[K     |████████████████████████████████| 3.4 MB 1.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tensorflow<2.5,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text==2.4.*) (2.4.1)\nRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text==2.4.*) (0.12.0)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.15.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.1.2)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (3.3.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (0.36.2)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (0.3.3)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.1.0)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (2.4.1)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.6.3)\nRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (2.10.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (0.12.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (3.15.8)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (3.7.4.3)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (2.4.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.12)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.12.1)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.32.0)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.19.5)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (0.2.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.0.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.26.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (0.4.3)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.8.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (2.25.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (3.3.4)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (49.6.0.post20210108)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (4.7.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (4.2.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (3.4.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (0.4.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (1.26.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (2020.12.5)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (3.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text==2.4.*) (3.4.1)\nInstalling collected packages: tensorflow-text\nSuccessfully installed tensorflow-text-2.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T11:48:35.391732Z","iopub.execute_input":"2021-05-31T11:48:35.392068Z","iopub.status.idle":"2021-05-31T11:48:35.410148Z","shell.execute_reply.started":"2021-05-31T11:48:35.392031Z","shell.execute_reply":"2021-05-31T11:48:35.409383Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import string\nimport re\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport tensorflow_addons as tfa\nfrom official.nlp import optimization\nfrom official.nlp.bert import tokenization\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:35.412982Z","iopub.execute_input":"2021-05-31T11:48:35.413230Z","iopub.status.idle":"2021-05-31T11:48:41.031082Z","shell.execute_reply.started":"2021-05-31T11:48:35.413206Z","shell.execute_reply":"2021-05-31T11:48:41.030179Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df= pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df= pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample_sub_df= pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:41.032416Z","iopub.execute_input":"2021-05-31T11:48:41.032833Z","iopub.status.idle":"2021-05-31T11:48:41.298651Z","shell.execute_reply.started":"2021-05-31T11:48:41.032796Z","shell.execute_reply":"2021-05-31T11:48:41.297793Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"samples = ['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n '#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires',\n '#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas',\n \"I'm on top of the hill and I can see a fire in the woods...\",\n 'Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding',\n \"#raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count \",\n '#Flood in Bago Myanmar #We arrived Bago',\n 'Damage to school bus on 80 in multi car crash #BREAKING ',\n 'What a goooooooaaaaaal!!!!!!',\n 'this is ridiculous....',\n 'What a wonderful day!',\n \"No way...I can't eat that shit\",\n 'Was in NYC last week!',\n 'Love my girlfriend',\n 'Cooool :)',\n '@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C',\n 'We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw']\n\ndef clean_tweet(tweet):\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # remove html\n    tweet = re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', tweet)        \n    # remove special chars \n    # repr to convert to raw string in order to handle this case\n    tweet = re.sub(r\"\\\\x\\d\\d\\w\\w\",'', repr(tweet))\n    # remove url\n    tweet = re.sub('https?://\\S+|www\\.\\S+', '', tweet)\n\n    tweet = re.sub('[%s]' % re.escape(string.punctuation), '', tweet)\n    tweet = re.sub('\\n', '', tweet)\n    tweet = tweet.lower()\n    \n    return tweet\n    \n\nlist(map(clean_tweet, samples))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:41.300009Z","iopub.execute_input":"2021-05-31T11:48:41.300397Z","iopub.status.idle":"2021-05-31T11:48:41.315166Z","shell.execute_reply.started":"2021-05-31T11:48:41.300359Z","shell.execute_reply":"2021-05-31T11:48:41.314230Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['our deeds are the reason of this earthquake may allah forgive us all',\n 'rockyfire update  california hwy 20 closed in both directions due to lake county fire  cafire wildfires',\n 'flood disaster heavy rain causes flash flooding of streets in manitou colorado springs areas',\n 'im on top of the hill and i can see a fire in the woods',\n 'haha south tampa is getting flooded hah wait a second i live in south tampa what am i gonna do what am i gonna do fvck flooding',\n 'raining flooding florida tampabay tampa 18 or 19 days ive lost count ',\n 'flood in bago myanmar we arrived bago',\n 'damage to school bus on 80 in multi car crash breaking ',\n 'what a goooooooaaaaaal',\n 'this is ridiculous',\n 'what a wonderful day',\n 'no wayi cant eat that shit',\n 'was in nyc last week',\n 'love my girlfriend',\n 'cooool ',\n 'bbcmtd wholesale markets ablaze ',\n 'we always try to bring the heavy metal rt ']"},"metadata":{}}]},{"cell_type":"code","source":"def add_loc_keyword(df):\n    df.location.fillna('', inplace=True)\n    df.keyword.fillna('', inplace=True)\n    df.loc[:, 'text_ctx'] = df.location+\" \"+df.keyword + \" \" + df.text\n    \n    return df\n\nadd_loc_keyword(train_df)\nadd_loc_keyword(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:41.316802Z","iopub.execute_input":"2021-05-31T11:48:41.317457Z","iopub.status.idle":"2021-05-31T11:48:41.370480Z","shell.execute_reply.started":"2021-05-31T11:48:41.317414Z","shell.execute_reply":"2021-05-31T11:48:41.369700Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         id keyword location  \\\n0         0                    \n1         2                    \n2         3                    \n3         9                    \n4        11                    \n...     ...     ...      ...   \n3258  10861                    \n3259  10865                    \n3260  10868                    \n3261  10874                    \n3262  10875                    \n\n                                                   text  \\\n0                    Just happened a terrible car crash   \n1     Heard about #earthquake is different cities, s...   \n2     there is a forest fire at spot pond, geese are...   \n3              Apocalypse lighting. #Spokane #wildfires   \n4         Typhoon Soudelor kills 28 in China and Taiwan   \n...                                                 ...   \n3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n3259  Storm in RI worse than last hurricane. My city...   \n3260  Green Line derailment in Chicago http://t.co/U...   \n3261  MEG issues Hazardous Weather Outlook (HWO) htt...   \n3262  #CityofCalgary has activated its Municipal Eme...   \n\n                                               text_ctx  \n0                    Just happened a terrible car crash  \n1       Heard about #earthquake is different cities,...  \n2       there is a forest fire at spot pond, geese a...  \n3              Apocalypse lighting. #Spokane #wildfires  \n4         Typhoon Soudelor kills 28 in China and Taiwan  \n...                                                 ...  \n3258    EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FAS...  \n3259    Storm in RI worse than last hurricane. My ci...  \n3260    Green Line derailment in Chicago http://t.co...  \n3261    MEG issues Hazardous Weather Outlook (HWO) h...  \n3262    #CityofCalgary has activated its Municipal E...  \n\n[3263 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>text_ctx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>Just happened a terrible car crash</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td></td>\n      <td></td>\n      <td>Heard about #earthquake is different cities, s...</td>\n      <td>Heard about #earthquake is different cities,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td></td>\n      <td></td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n      <td>there is a forest fire at spot pond, geese a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td></td>\n      <td></td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td></td>\n      <td></td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td></td>\n      <td></td>\n      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FAS...</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td></td>\n      <td></td>\n      <td>Storm in RI worse than last hurricane. My city...</td>\n      <td>Storm in RI worse than last hurricane. My ci...</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td></td>\n      <td></td>\n      <td>Green Line derailment in Chicago http://t.co/U...</td>\n      <td>Green Line derailment in Chicago http://t.co...</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td></td>\n      <td></td>\n      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n      <td>MEG issues Hazardous Weather Outlook (HWO) h...</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td></td>\n      <td></td>\n      <td>#CityofCalgary has activated its Municipal Eme...</td>\n      <td>#CityofCalgary has activated its Municipal E...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tweets_train_val = train_df.text_ctx.apply(clean_tweet).values.tolist()\ntweets_test = test_df.text_ctx.apply(clean_tweet).values.tolist()\nlabels_train_val = train_df.target.values\n\nfrom sklearn.model_selection import train_test_split\ntweets_train, tweets_val, y_train, y_val = train_test_split(tweets_train_val, labels_train_val, test_size=0.20, random_state=42, shuffle=True)\nnp.unique(y_train, return_counts=True)[1], np.unique(y_val, return_counts=True)[1]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:41.372678Z","iopub.execute_input":"2021-05-31T11:48:41.373024Z","iopub.status.idle":"2021-05-31T11:48:41.764130Z","shell.execute_reply.started":"2021-05-31T11:48:41.372979Z","shell.execute_reply":"2021-05-31T11:48:41.763290Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(array([3468, 2622]), array([874, 649]))"},"metadata":{}}]},{"cell_type":"code","source":"neg, pos = np.bincount(y_train)\ntotal = neg + pos\ninitial_bias = np.log([pos/neg])\ninitial_bias, pos/neg\n\n\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:41.765621Z","iopub.execute_input":"2021-05-31T11:48:41.765968Z","iopub.status.idle":"2021-05-31T11:48:41.776310Z","shell.execute_reply.started":"2021-05-31T11:48:41.765931Z","shell.execute_reply":"2021-05-31T11:48:41.775345Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([0.87802768, 1.16132723])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Full bert model using raw classes","metadata":{}},{"cell_type":"code","source":"bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n\ndef get_pre_process_layer():\n    pre_process_tfhub_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n    bert_preprocess_layer = hub.KerasLayer(pre_process_tfhub_url)\n    return bert_preprocess_layer\n    \n    \n    \nsm_tweets = [\"the dog\",\n            \"this is a new 2000 @jk http://www.news.com \\U0001F600 !- johndoe@acme.org“) \",\n           \"#go https://www.ls.ch:80/home?lan=fr long life \\x89ÛÒ to sport in 2021 #football HOP LS not me !-)) ?!!!\"]\n\ntweets_preprocessed = get_pre_process_layer()(sm_tweets)\n\nprint(f'Keys       : {list(tweets_preprocessed.keys())}')\nprint(f'Shape      : {tweets_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {tweets_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {tweets_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {tweets_preprocessed[\"input_type_ids\"][0, :12]}')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:41.777849Z","iopub.execute_input":"2021-05-31T11:48:41.778201Z","iopub.status.idle":"2021-05-31T11:48:46.632265Z","shell.execute_reply.started":"2021-05-31T11:48:41.778163Z","shell.execute_reply":"2021-05-31T11:48:46.631442Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\nShape      : (3, 128)\nWord Ids   : [ 101 1996 3899  102    0    0    0    0    0    0    0    0]\nInput Mask : [1 1 1 1 0 0 0 0 0 0 0 0]\nType Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_bert_encoder_layer(trainable=True):\n    encoder_tfhub_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n    encoder = hub.KerasLayer(encoder_tfhub_url, trainable=True, name='BERT_encoder')\n    return encoder\n\ntweets_encoded = get_bert_encoder_layer(trainable=True)(tweets_preprocessed)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:48:46.633638Z","iopub.execute_input":"2021-05-31T11:48:46.633976Z","iopub.status.idle":"2021-05-31T11:48:55.334611Z","shell.execute_reply.started":"2021-05-31T11:48:46.633939Z","shell.execute_reply":"2021-05-31T11:48:55.333747Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x7f25f40ed830>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 208, in __del__\n    self._destroy_resource()\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 253, in restored_function_body\n    return _call_concrete_function(function, inputs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 75, in _call_concrete_function\n    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 116, in _call_flat\n    cancellation_manager)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1932, in _call_flat\n    flat_outputs = forward_function.call(ctx, args_with_tangents)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 589, in call\n    executor_type=executor_type)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 1206, in partitioned_call\n    f.add_to_graph(graph)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 505, in add_to_graph\n    g._add_function(self)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3396, in _add_function\n    gradient)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tweets_encoded.keys())\ntweets_encoded.get('pooled_output')[0]","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:49:56.476849Z","iopub.execute_input":"2021-05-31T11:49:56.477240Z","iopub.status.idle":"2021-05-31T11:49:56.494018Z","shell.execute_reply.started":"2021-05-31T11:49:56.477207Z","shell.execute_reply":"2021-05-31T11:49:56.493033Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"dict_keys(['sequence_output', 'pooled_output', 'default', 'encoder_outputs'])\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(512,), dtype=float32, numpy=\narray([ 9.90585923e-01,  9.02491271e-01,  2.33190194e-01,  1.05213933e-01,\n       -5.19300282e-01,  8.29919398e-01,  9.98373926e-01, -6.79914653e-01,\n       -6.35009408e-01, -9.91357923e-01,  4.23507877e-02, -9.95681524e-01,\n       -9.83761191e-01, -9.71929431e-01, -3.39588493e-01,  2.84557790e-01,\n        1.38770446e-01,  7.46254772e-02, -7.04332411e-01,  5.19972920e-01,\n        1.26761541e-01, -2.72015870e-01,  4.01239395e-01,  6.78625226e-01,\n        9.96361077e-01, -5.85017204e-01, -3.80504727e-01,  4.98764142e-02,\n       -1.03830164e-02,  6.34233713e-01,  9.20646548e-01,  1.53623939e-01,\n        4.59918112e-01, -1.60974070e-01, -1.52723566e-01, -1.32511869e-01,\n       -4.70786482e-01, -3.36184621e-01,  1.01673439e-01, -1.28829017e-01,\n        2.97841877e-01, -4.44863796e-01,  1.17724992e-01, -7.71950722e-01,\n        3.24944556e-02,  3.57039601e-01, -9.96733367e-01,  1.93455428e-01,\n        5.14397144e-01, -7.15311766e-01,  9.95311320e-01,  1.35183781e-01,\n        9.81824934e-01, -2.94035047e-01,  7.29843676e-01,  2.72521704e-01,\n       -3.13018970e-02, -5.33734977e-01, -9.99380648e-01, -9.92213488e-01,\n        2.41097659e-01,  2.39483550e-01,  7.59007514e-01,  3.33283067e-01,\n       -2.91676253e-01, -5.49172163e-01,  9.98237371e-01,  2.71996230e-01,\n       -4.56830740e-01,  9.74104702e-01, -4.48889554e-01,  9.99806821e-01,\n       -5.21555364e-01, -5.50982594e-01,  8.65195543e-02, -3.01649738e-02,\n        1.55885983e-02,  5.91757476e-01, -1.13417678e-01, -5.50843537e-01,\n       -9.87057447e-01,  2.88789034e-01,  8.19353282e-01,  5.70314586e-01,\n        9.82018590e-01, -9.87676442e-01, -1.12820886e-01,  5.16472697e-01,\n       -4.90494221e-01,  5.29665887e-01, -9.90643442e-01,  6.88801587e-01,\n        2.29100361e-01, -4.57103729e-01, -9.84716594e-01,  9.54252899e-01,\n       -2.69001842e-01,  2.67942339e-01, -9.94766116e-01,  2.78896123e-01,\n       -9.43484604e-01, -9.99749601e-01, -8.46782029e-01,  2.33316034e-01,\n       -4.95577812e-01, -2.31499702e-01,  9.93657947e-01, -1.71576351e-01,\n       -4.11218226e-01,  8.13936949e-01,  1.07117184e-01,  3.01225007e-01,\n        9.91350889e-01, -9.92626250e-01, -2.46343225e-01,  4.34509069e-01,\n       -6.30963027e-01,  9.54479694e-01,  1.95512399e-01,  9.98108327e-01,\n       -9.59761262e-01, -4.41734016e-01, -9.96526659e-01,  5.48093677e-01,\n       -4.25607115e-01, -3.76274407e-01, -5.99950016e-01, -3.46208036e-01,\n       -3.81268919e-01, -3.41465741e-01,  9.92069066e-01,  9.86269474e-01,\n        9.64741707e-01,  4.41966520e-04,  4.76669371e-01, -5.80419779e-01,\n        9.72579062e-01, -9.96906042e-01, -9.98897076e-01,  4.43651788e-02,\n        9.99456584e-01, -1.33957103e-01, -5.67628026e-01, -3.66618037e-01,\n       -5.08184955e-02,  9.89876032e-01, -9.78041112e-01,  9.56863940e-01,\n       -9.71651316e-01,  1.36364922e-01, -4.76359069e-01, -2.10309237e-01,\n       -7.24271592e-03, -4.98444527e-01,  3.42844516e-01, -2.89976716e-01,\n       -6.51518479e-02,  7.30903804e-01,  3.11229646e-01,  2.23301873e-01,\n       -4.19523835e-01,  6.43022537e-01, -5.04335880e-01, -5.41124761e-01,\n        9.99989629e-01, -4.25359070e-01, -3.77521306e-01, -5.30419886e-01,\n        9.87840712e-01,  6.77065134e-01,  1.16824673e-03, -9.95203555e-01,\n       -8.99389803e-01, -4.72184509e-01,  6.17065430e-01,  3.71584237e-01,\n       -8.20817828e-01,  3.21965605e-01,  9.99913514e-01,  6.70192659e-01,\n        9.46367145e-01, -7.17248544e-02,  9.97553110e-01, -2.90262818e-01,\n        8.75496626e-01, -9.96832550e-01, -9.96919453e-01, -1.23472705e-01,\n        3.48070025e-01, -6.81521967e-02,  9.72064912e-01, -9.69098568e-01,\n       -8.45256388e-01, -5.77851295e-01,  9.98426914e-01, -2.20132872e-01,\n       -6.92567229e-01, -9.88824606e-01,  4.93437707e-01,  6.56546950e-02,\n        7.45770782e-02, -5.98846734e-01, -6.93504393e-01,  3.39564383e-01,\n        2.76636034e-01,  9.98639226e-01, -5.97975433e-01,  9.98030782e-01,\n        9.95399892e-01, -2.25663975e-01,  6.91204369e-01,  8.82397115e-01,\n        1.41735435e-01, -8.68653581e-02,  9.91639316e-01, -9.93705809e-01,\n       -7.93870211e-01,  8.78656924e-01,  8.95860791e-01,  2.20538765e-01,\n       -9.93094802e-01, -4.71729308e-01, -9.78573263e-01,  3.50257486e-01,\n        5.61963737e-01,  3.62653881e-01, -9.95652318e-01,  1.56358093e-01,\n       -1.67055234e-01,  9.78548229e-01,  2.22906932e-01,  8.11704397e-02,\n       -1.15079716e-01,  2.52471536e-01,  9.94723260e-01,  2.24972725e-01,\n       -9.83588278e-01,  1.28694430e-01,  1.26689717e-01,  1.89854726e-01,\n        9.62435365e-01,  9.99974310e-01, -7.34294772e-01,  9.97436345e-01,\n        6.47264540e-01,  4.89976369e-02,  9.88512814e-01,  2.67601222e-01,\n        9.63396192e-01, -4.20355737e-01, -1.50757298e-01, -1.77522346e-01,\n        6.73161261e-03,  3.74748170e-01,  9.99876618e-01,  1.12639867e-01,\n       -8.34170207e-02, -9.95749652e-01,  1.26457168e-02, -4.49527770e-01,\n       -6.38553858e-01, -9.71508026e-01,  1.52031817e-02, -4.03026164e-01,\n       -1.33632720e-01,  9.91576433e-01,  9.97568309e-01, -9.33967054e-01,\n       -4.81956095e-01,  1.71431288e-01,  8.83426845e-01,  9.34752747e-02,\n       -9.50970769e-01,  9.99214113e-01, -2.83164024e-01, -9.01880622e-01,\n        9.99025822e-01,  4.69498873e-01, -4.20176685e-01,  3.06389958e-01,\n       -9.37015891e-01, -9.86396790e-01, -1.41577646e-01,  9.95670438e-01,\n        9.51008022e-01, -1.90710425e-01,  2.05602080e-01,  2.68127859e-01,\n       -9.99712169e-01, -9.98746872e-01, -5.71017683e-01,  2.94291198e-01,\n        2.19956845e-01,  9.91168261e-01,  9.89672422e-01, -6.92976654e-01,\n       -6.01222754e-01,  7.97488749e-01, -2.12089926e-01,  9.99300659e-01,\n        9.99870062e-01, -2.74644643e-01, -5.16695797e-01,  9.99048531e-01,\n       -3.02780658e-01,  2.27875248e-01, -5.73696077e-01,  9.87399399e-01,\n       -9.33520734e-01,  4.46812212e-01,  4.40810800e-01, -5.76102316e-01,\n       -9.22480971e-02,  7.47711539e-01,  8.83145928e-01, -2.22265333e-01,\n       -2.09095478e-02,  3.61486286e-01,  1.85247123e-01,  9.99852717e-01,\n        3.24376523e-02, -9.81506646e-01,  1.99770853e-01, -6.35583401e-01,\n        1.31251961e-01,  8.07233527e-02, -5.40756404e-01,  7.34419882e-01,\n       -9.66522932e-01,  9.14755702e-01, -5.00657916e-01, -2.26542652e-01,\n       -3.28272462e-01,  9.41437304e-01,  4.42796797e-01,  1.81729302e-01,\n       -4.40089643e-01, -9.85506713e-01, -4.75519657e-01,  1.75185949e-01,\n        9.21031296e-01, -2.80053820e-02, -6.83426261e-01,  9.93966997e-01,\n       -9.80878845e-02,  7.06757784e-01,  7.82995880e-01,  9.96963024e-01,\n        9.88157764e-02,  9.67602134e-01, -7.44126663e-02,  1.82816282e-01,\n        5.24385311e-02, -1.87632740e-01,  8.52340341e-01, -2.90355653e-01,\n       -8.19845080e-01, -3.98909122e-01, -3.59669566e-01,  5.24260961e-02,\n       -9.62397158e-01, -7.05972552e-01, -6.51478648e-01,  2.32163250e-01,\n        1.31441839e-02, -9.90174055e-01, -8.04390132e-01, -4.35318649e-01,\n        3.14736843e-01,  6.47660494e-01, -1.38220608e-01,  9.35391665e-01,\n       -1.89837754e-01,  3.83818507e-01,  8.25316668e-01, -9.97488201e-01,\n        3.27588767e-01,  2.01612398e-01, -1.27512366e-01, -3.42552453e-01,\n        2.44811520e-01,  5.42532027e-01, -9.89603102e-01,  2.69542992e-01,\n        9.65805173e-01, -8.57271194e-01,  5.03483951e-01, -4.71509129e-01,\n        6.09505355e-01, -4.10529166e-01,  9.76879299e-02, -4.72094089e-01,\n       -9.99972522e-01,  2.51260519e-01, -3.53370279e-01, -3.17219257e-01,\n        1.05220385e-01, -1.02907196e-02,  8.79506469e-01, -2.46830121e-01,\n        2.53864765e-01, -9.79165196e-01, -3.71429622e-01, -6.11879071e-03,\n        9.56703484e-01, -6.80657923e-02, -1.68585345e-01,  3.91853660e-01,\n       -2.24034354e-01, -9.96769488e-01,  9.96657670e-01,  9.96793687e-01,\n       -9.90398288e-01,  4.55631495e-01, -9.68031824e-01,  4.22813207e-01,\n        5.76763213e-01, -5.99633336e-01, -7.89598227e-02, -9.96596932e-01,\n       -4.52688597e-02,  9.95649993e-01,  2.87359715e-01,  9.69315052e-01,\n        1.27063110e-01, -9.98108447e-01, -2.60709047e-01, -1.89843029e-01,\n        1.82317287e-01,  4.33840826e-02,  5.62763572e-01,  3.37626159e-01,\n        9.99277115e-01, -9.89124417e-01, -9.53354776e-01,  5.31920612e-01,\n       -8.13452065e-01, -7.38737643e-01,  4.03171182e-01, -1.58862635e-01,\n        8.12812895e-02, -9.93386924e-01, -7.80278385e-01,  2.66672194e-01,\n       -9.95142758e-01, -6.34859920e-01, -9.55638945e-01, -2.78418779e-01,\n       -1.26577690e-01,  5.02313554e-01,  7.76391774e-02, -1.43871814e-01,\n       -2.52240002e-01,  9.87770915e-01,  5.39410830e-01,  9.97314632e-01,\n       -1.88851487e-02, -2.45637432e-01,  3.21667314e-01,  9.58670020e-01,\n       -2.86892593e-01, -9.20261070e-02, -9.98343229e-01,  7.78680563e-01,\n        9.78945076e-01,  9.99017239e-01,  3.53849173e-01,  9.90076840e-01,\n       -9.97270048e-01, -2.63029218e-01,  4.45740432e-01, -9.65946853e-01,\n        9.14248347e-01, -4.20835838e-02,  3.75873774e-01, -9.97053921e-01,\n       -9.95316386e-01,  1.03512667e-01, -9.97892797e-01,  7.17412591e-01,\n        5.33936322e-01,  4.02236402e-01,  7.13239133e-01,  4.98681754e-01,\n        6.19862974e-02, -9.97889400e-01, -9.74089622e-01, -2.29069412e-01,\n       -3.49275261e-01, -3.88182193e-01, -3.78002465e-01, -9.99981761e-01,\n       -8.18017244e-01,  8.76563072e-01, -1.27549702e-02, -3.30315709e-01,\n        2.50945777e-01,  2.02210486e-01, -9.86613154e-01,  9.99990165e-01,\n       -9.69514251e-01, -1.87877417e-01, -1.70490462e-02,  1.10507816e-01,\n        7.05381706e-02,  3.70086640e-01,  9.85148728e-01, -9.40028131e-01,\n       -2.94257283e-01,  1.78922534e-01, -4.06637222e-01, -3.55144739e-01],\n      dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = np.array(tweets_train)\nX_val = np.array(tweets_val)\nX_test = np.array(tweets_test)\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\ntest_ds = tf.data.Dataset.from_tensor_slices(X_test)\n\ntrain_ds = train_ds.cache()\nval_ds = val_ds.cache()\ntest_ds = test_ds.cache()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:51:18.906175Z","iopub.execute_input":"2021-05-31T11:51:18.906632Z","iopub.status.idle":"2021-05-31T11:51:18.930362Z","shell.execute_reply.started":"2021-05-31T11:51:18.906586Z","shell.execute_reply":"2021-05-31T11:51:18.929549Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"mpl.rcParams['figure.figsize'] = (8, 6)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\ndef plot_metric(history, label, metric, n):\n# Use a log scale on y-axis to show the wide range of values.\n  plt.plot(history.epoch, history.history[metric],\n               color=colors[n], label='Train ' + label)\n  plt.plot(history.epoch, history.history['val_'+metric],\n               color=colors[n], label='Val ' + label,\n               linestyle=\"--\")\n  plt.legend()\n  plt.xlabel('Epoch')\n  plt.ylabel(metric)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:51:23.569881Z","iopub.execute_input":"2021-05-31T11:51:23.570225Z","iopub.status.idle":"2021-05-31T11:51:23.576340Z","shell.execute_reply.started":"2021-05-31T11:51:23.570196Z","shell.execute_reply":"2021-05-31T11:51:23.575246Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"THRESHOLD=0.5\nMETRICS = [        \n    tfa.metrics.F1Score(num_classes=2, average='micro', threshold=THRESHOLD)    \n]\n\n\ndef build_classifier_model(pre_processing_layer=None, bert_layer=None, max_len=128, metrics=None, optim=None, output_bias=None, \n                           lr=1e-3, do_dropout=False, do_batch_norm=False, dropout=0.5):\n    \n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n    \n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')    \n    encoder_inputs = pre_processing_layer(text_input)        \n    outputs = bert_layer(encoder_inputs)\n    clf_output = outputs['pooled_output'] #[:, 0, :]\n    out = tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)(clf_output)\n    \n    model = tf.keras.Model(inputs=text_input, outputs=out)\n    model.compile(optim, loss='binary_crossentropy', metrics=metrics)\n    \n    return model\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:52:48.550200Z","iopub.execute_input":"2021-05-31T11:52:48.550562Z","iopub.status.idle":"2021-05-31T11:52:48.569186Z","shell.execute_reply.started":"2021-05-31T11:52:48.550525Z","shell.execute_reply":"2021-05-31T11:52:48.568310Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))\nstrategy = tf.distribute.TPUStrategy(resolver)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:52:54.027145Z","iopub.execute_input":"2021-05-31T11:52:54.027635Z","iopub.status.idle":"2021-05-31T11:52:54.355395Z","shell.execute_reply.started":"2021-05-31T11:52:54.027589Z","shell.execute_reply":"2021-05-31T11:52:54.353640Z"}}},{"cell_type":"code","source":"# try trainable = true/false    \nBATCH_SIZE=64\noptim = opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n\ncbs = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, verbose=1),\n       tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.001,verbose=1),\n       tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)]\n#with strategy.scope():\nmodel = build_classifier_model(pre_processing_layer=get_pre_process_layer(), bert_layer=get_bert_encoder_layer(trainable=True),\n                                      metrics=METRICS, optim=optim, output_bias=initial_bias, lr=1e-3, do_dropout=False, do_batch_norm=False)\nhistory = model.fit(\n    train_ds.batch(BATCH_SIZE).prefetch(1), \n    validation_data=val_ds.batch(BATCH_SIZE),\n    epochs=20,    \n    callbacks=cbs,\n    verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:54:38.067575Z","iopub.execute_input":"2021-05-31T11:54:38.067973Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n96/96 [==============================] - 45s 421ms/step - loss: 0.5822 - f1_score: 0.5977 - val_loss: 0.4456 - val_f1_score: 0.7703\nEpoch 2/20\n96/96 [==============================] - 39s 407ms/step - loss: 0.4339 - f1_score: 0.7638 - val_loss: 0.4255 - val_f1_score: 0.7882\nEpoch 3/20\n96/96 [==============================] - 39s 406ms/step - loss: 0.3925 - f1_score: 0.7868 - val_loss: 0.4236 - val_f1_score: 0.7884\nEpoch 4/20\n96/96 [==============================] - 39s 408ms/step - loss: 0.3607 - f1_score: 0.8082 - val_loss: 0.4229 - val_f1_score: 0.7871\nEpoch 5/20\n96/96 [==============================] - 39s 406ms/step - loss: 0.3260 - f1_score: 0.8338 - val_loss: 0.4293 - val_f1_score: 0.7914\nEpoch 6/20\n96/96 [==============================] - 39s 411ms/step - loss: 0.2960 - f1_score: 0.8506 - val_loss: 0.4473 - val_f1_score: 0.7888\nEpoch 7/20\n96/96 [==============================] - 40s 414ms/step - loss: 0.2704 - f1_score: 0.8676 - val_loss: 0.4564 - val_f1_score: 0.7927\nEpoch 8/20\n96/96 [==============================] - 39s 406ms/step - loss: 0.2442 - f1_score: 0.8810 - val_loss: 0.4933 - val_f1_score: 0.7820\nEpoch 9/20\n96/96 [==============================] - 39s 410ms/step - loss: 0.2147 - f1_score: 0.8976 - val_loss: 0.5097 - val_f1_score: 0.7887\nEpoch 10/20\n96/96 [==============================] - 39s 406ms/step - loss: 0.1831 - f1_score: 0.9196 - val_loss: 0.5572 - val_f1_score: 0.7799\nEpoch 11/20\n96/96 [==============================] - 40s 413ms/step - loss: 0.1588 - f1_score: 0.9301 - val_loss: 0.6910 - val_f1_score: 0.7614\nEpoch 12/20\n96/96 [==============================] - 39s 407ms/step - loss: 0.1479 - f1_score: 0.9324 - val_loss: 0.6520 - val_f1_score: 0.7670\nEpoch 13/20\n39/96 [===========>..................] - ETA: 20s - loss: 0.1150 - f1_score: 0.9452","output_type":"stream"}]},{"cell_type":"code","source":"plot_metric(history, \"Bert\",'loss', 0)\nplot_metric(history, \"RNN\",'f1_score', 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T13:33:43.692606Z","iopub.status.idle":"2021-05-28T13:33:43.693049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_prob = model.predict(test_ds.batch(64))\nthreshold = 0.5\ntest_pred = np.where(test_pred_prob > threshold, 1,0)\ntest_pred = np.squeeze(test_pred)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T11:55:00.888307Z","iopub.execute_input":"2021-05-28T11:55:00.888656Z","iopub.status.idle":"2021-05-28T11:55:13.365929Z","shell.execute_reply.started":"2021-05-28T11:55:00.888626Z","shell.execute_reply":"2021-05-28T11:55:13.365055Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"zip((test_df.id.values, test_pred))\n\ntest_ids = test_df.id.values\ntest_pred_df = pd.DataFrame({'id':test_ids, 'target':test_pred})\ntest_pred_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T11:55:16.726887Z","iopub.execute_input":"2021-05-28T11:55:16.727234Z","iopub.status.idle":"2021-05-28T11:55:16.865942Z","shell.execute_reply.started":"2021-05-28T11:55:16.727186Z","shell.execute_reply":"2021-05-28T11:55:16.865140Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"do_small_data_fit = True\n\nif do_small_data_fit:\n    def create_small_data(X,y, nb_per_class=1):\n        true_idxs = np.where(y == True)[0][:nb_per_class]\n        false_idxs = np.where(y == False)[0][:nb_per_class]        \n        idxs = np.concatenate((true_idxs, false_idxs)).tolist()\n        X_sm = X[idxs]\n        y_sm = y[idxs]\n        \n        return X_sm, y_sm\n\n    # try with 2 and 10 samples\n    SM_NB=10\n    X_train_sm, y_train_sm = create_small_data(X_train, y_train, nb_per_class=int(SM_NB/2))\n    \n    optim = opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    sm_model = build_classifier_model(pre_processing_layer=get_pre_process_layer(), bert_layer=get_bert_encoder_layer(trainable=True),\n                                      metrics=METRICS, optim=optim, output_bias=initial_bias, lr=1e-3, do_dropout=False, do_batch_norm=False)\n    sm_history = sm_model.fit(\n        X_train_sm,\n        y_train_sm,\n        batch_size=SM_NB,\n        epochs=50,\n        verbose=0)\n\n    threshold = 0.5\n    sm_pred_prob = sm_model.predict(X_train_sm)\n    sm_pred = np.where(sm_pred_prob > threshold, 1,0)\n    print(\"pred probabilities: \")\n    print(sm_pred_prob)\n    print(\"pred:\")\n    print(sm_pred)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T11:33:38.711418Z","iopub.execute_input":"2021-05-28T11:33:38.711738Z","iopub.status.idle":"2021-05-28T11:33:54.836398Z","shell.execute_reply.started":"2021-05-28T11:33:38.711708Z","shell.execute_reply":"2021-05-28T11:33:54.833830Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"pred probabilities: \n[[0.9973653 ]\n [0.99668175]\n [0.99776864]\n [0.99774987]\n [0.99655664]\n [0.00431994]\n [0.00288992]\n [0.00506494]\n [0.00507691]\n [0.00284994]]\npred:\n[[1]\n [1]\n [1]\n [1]\n [1]\n [0]\n [0]\n [0]\n [0]\n [0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_tokens.shape, sample_masks, sample_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmax_len = SEQ_LEN\ninput_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\noutput, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_layer([sample_tokens[:], sample_masks[:], sample_ids[:]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Small bert","metadata":{}},{"cell_type":"code","source":"THRESHOLD=0.5\nMETRICS = [        \n    tfa.metrics.F1Score(num_classes=2, average='micro', threshold=THRESHOLD)    \n]\n\n\ndef build_classifier_model(metrics=None, optim=None, output_bias=None, lr=1e-3, do_dropout=False, do_batch_norm=False, dropout=0.5):\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    x = outputs['pooled_output']    \n    #x = tf.keras.layers.Dropout(0.1)(x)\n    \n    # also include batchnorm\n    #tf.keras.layers.Dense(120),\n    #    tf.keras.layers.BatchNormalization(),\n    #    tf.keras.layers.Activation('sigmoid'),\n    \n    # try different initializers\n\n    # clean texts\n    \n    x = tf.keras.layers.Dense(256)(x)\n    if do_batch_norm is True:\n        x = tf.keras.layers.BatchNormalization()(x)\n    if do_dropout is True:\n        x = tf.keras.layers.Dropout(dropout)(x)\n    x = tf.keras.activations.relu(x)\n    \n    '''\n    x = tf.keras.layers.Dense(128)(x)\n    if do_batch_norm is True:\n        x = tf.keras.layers.BatchNormalization()(x)\n    if do_dropout is True:\n        x = tf.keras.layers.Dropout(dropout)(x)\n    x = tf.keras.activations.relu(x)        \n    \n    x = tf.keras.layers.Dense(64)(x)\n    if do_batch_norm is True:\n        x = tf.keras.layers.BatchNormalization()(x)\n    if do_dropout is True:\n        x = tf.keras.layers.Dropout(dropout)(x)\n    x = tf.keras.activations.relu(x)\n    '''\n    x = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(x)\n    model =  tf.keras.Model(text_input, x)\n    if optim is None:\n        optim = tf.keras.optimizers.Adam(lr=lr)\n        \n    model.compile(optimizer= optim,\n                  loss=tf.keras.losses.BinaryCrossentropy(),\n                  metrics=metrics)\n    \n    return model\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train model on full data","metadata":{}},{"cell_type":"code","source":"EPOCH_NB=60\n\nsteps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\nnum_train_steps = steps_per_epoch * EPOCH_NB\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\nadamw = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nBATCH_SIZE = 64\n\ncbs = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, verbose=1),\n      tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.001,verbose=1)]\n\nmodel = build_classifier_model(metrics=METRICS, optim=adamw, output_bias=initial_bias, lr=3e-5, do_dropout=True, do_batch_norm=False)\n\ntrain_ds = train_ds.batch(BATCH_SIZE)\nval_ds = val_ds.batch(BATCH_SIZE)\n\nhistory = model.fit(x=train_ds,\n                    validation_data=val_ds,\n                    epochs=EPOCH_NB,\n                    callbacks=cbs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test prediction\n","metadata":{}},{"cell_type":"code","source":"#!rm submission.csv\n#!ls -la","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test model on small dataset","metadata":{}},{"cell_type":"code","source":"do_small_data_fit = True\n\nif do_small_data_fit:\n    def create_small_data(X,y, nb_per_class=1):\n        true_idxs = np.where(y == True)[0][:nb_per_class]\n        false_idxs = np.where(y == False)[0][:nb_per_class]        \n        idxs = np.concatenate((true_idxs, false_idxs)).tolist()           \n        X_sm = X[idxs]\n        y_sm = y[idxs]\n        return X_sm, y_sm\n\n    # try with 2 and 10 samples\n    SM_NB=10\n    X_train_sm, y_train_sm = create_small_data(X_train, y_train, nb_per_class=int(SM_NB/2))\n\n\n    sm_model = build_classifier_model(metrics=METRICS, optim=None, output_bias=initial_bias, lr=3e-5, do_dropout=False, do_batch_norm=False)\n    sm_history = sm_model.fit(\n        X_train_sm,\n        y_train_sm,\n        batch_size=SM_NB,\n        epochs=50,\n        verbose=0)\n\n    threshold = 0.5\n    sm_pred_prob = sm_model.predict(X_train_sm)\n    sm_pred = np.where(sm_pred_prob > threshold, 1,0)\n    print(\"pred probabilities: \")\n    print(sm_pred_prob)\n    print(\"pred:\")\n    print(sm_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# old","metadata":{}},{"cell_type":"code","source":"module_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n#\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n#'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8'\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\nbert_tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T11:56:52.844551Z","iopub.execute_input":"2021-05-26T11:56:52.845186Z","iopub.status.idle":"2021-05-26T11:57:01.120550Z","shell.execute_reply.started":"2021-05-26T11:56:52.845131Z","shell.execute_reply":"2021-05-26T11:57:01.119665Z"}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def encode_for_bert(texts, tokenizer, max_len=128):\n    \n    tokens = []\n    masks = []\n    segments = []\n    \n    \n    for text in texts:\n        text_tokens = tokenizer.tokenize(text)\n\n        text_tokens = text_tokens[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n\n        token_seq = tokenizer.convert_tokens_to_ids(input_sequence)\n        token_seq += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        tokens.append(token_seq)\n        masks.append(pad_masks)\n        segments.append(segment_ids)\n        \n\n    return np.array(tokens), np.array(masks), np.array(segments)\n    \n    \n    \nSEQ_LEN = 100\nsample_tokens, sample_masks, sample_ids = encode_for_bert(samples[:2], bert_tokenizer, max_len=SEQ_LEN)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T11:57:01.121894Z","iopub.execute_input":"2021-05-26T11:57:01.122267Z","iopub.status.idle":"2021-05-26T11:57:01.133407Z","shell.execute_reply.started":"2021-05-26T11:57:01.122205Z","shell.execute_reply":"2021-05-26T11:57:01.132384Z"}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"module_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n#\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n#'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8'\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\nbert_tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T11:56:52.844551Z","iopub.execute_input":"2021-05-26T11:56:52.845186Z","iopub.status.idle":"2021-05-26T11:57:01.120550Z","shell.execute_reply.started":"2021-05-26T11:56:52.845131Z","shell.execute_reply":"2021-05-26T11:57:01.119665Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def encode_for_bert(texts, tokenizer, max_len=128):\n    \n    tokens = []\n    masks = []\n    segments = []\n    \n    \n    for text in texts:\n        text_tokens = tokenizer.tokenize(text)\n\n        text_tokens = text_tokens[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n\n        token_seq = tokenizer.convert_tokens_to_ids(input_sequence)\n        token_seq += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        tokens.append(token_seq)\n        masks.append(pad_masks)\n        segments.append(segment_ids)\n        \n\n    return np.array(tokens), np.array(masks), np.array(segments)\n    \n    \n    \nSEQ_LEN = 100\nsample_tokens, sample_masks, sample_ids = encode_for_bert(samples[:2], bert_tokenizer, max_len=SEQ_LEN)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T11:57:01.121894Z","iopub.execute_input":"2021-05-26T11:57:01.122267Z","iopub.status.idle":"2021-05-26T11:57:01.133407Z","shell.execute_reply.started":"2021-05-26T11:57:01.122205Z","shell.execute_reply":"2021-05-26T11:57:01.132384Z"},"trusted":true},"execution_count":10,"outputs":[]}]}